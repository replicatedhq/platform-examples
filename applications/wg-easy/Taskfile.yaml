version: "3"

includes:
  utils: ./taskfiles/utils.yml
  dev: ./taskfiles/container.yml

vars:
  # Application configuration
  APP_SLUG: '{{.REPLICATED_APP | default "wg-easy-cre"}}'

  # Release configuration
  RELEASE_CHANNEL: '{{.RELEASE_CHANNEL | default "Unstable"}}'
  RELEASE_CHANNEL_ID: '{{.RELEASE_CHANNEL_ID}}'
  RELEASE_VERSION: '{{.RELEASE_VERSION | default "0.0.1"}}'
  RELEASE_NOTES: '{{.RELEASE_NOTES | default "Release created via task release-create"}}'
  REPLICATED_LICENSE_ID: '{{.REPLICATED_LICENSE_ID}}'

  # Cluster configuration
  CLUSTER_NAME: '{{.CLUSTER_NAME | default (printf "%s-cluster" (or (env "USER") "wg-easy-dev"))}}'
  K8S_VERSION: '{{.K8S_VERSION | default "1.32.2"}}'
  DISK_SIZE: '{{.DISK_SIZE | default "100"}}'
  INSTANCE_TYPE: '{{.INSTANCE_TYPE | default "r1.small"}}'
  DISTRIBUTION: '{{.DISTRIBUTION | default "k3s"}}'

  # Ports configuration
  EXPOSE_PORTS:
    - port: 30443
      protocol: https

  # GCP default configuration
  GCP_PROJECT: '{{.GCP_PROJECT | default "replicated-qa"}}'
  GCP_ZONE: '{{.GCP_ZONE | default "us-central1-a"}}'
  VM_NAME: '{{.VM_NAME | default (printf "%s-dev" (or (env "GUSER") "user"))}}'

  # Container workflow configuration
  # Available in GitHub Container Registry, Google Artifact Registry, and Replicated Registry
  DEV_CONTAINER_REGISTRY: '{{.DEV_CONTAINER_REGISTRY | default "ghcr.io"}}'
  DEV_CONTAINER_IMAGE: '{{.DEV_CONTAINER_IMAGE | default "replicatedhq/platform-examples/wg-easy-tools"}}'
  # Alternative registries:
  # - Google Artifact Registry: DEV_CONTAINER_REGISTRY=us-central1-docker.pkg.dev DEV_CONTAINER_IMAGE=replicated-qa/wg-easy/wg-easy-tools
  # - Replicated Registry: DEV_CONTAINER_REGISTRY=registry.replicated.com DEV_CONTAINER_IMAGE=wg-easy-cre/image
  # Container tags: "latest" for main branch, "{branch-name}" for feature branches, semver for releases
  # Override with DEV_CONTAINER_TAG=branch-name for feature branch containers or DEV_CONTAINER_TAG=v1.2.3 for releases
  DEV_CONTAINER_TAG: '{{.DEV_CONTAINER_TAG | default "latest"}}'
  DEV_CONTAINER_NAME: '{{.DEV_CONTAINER_NAME | default "wg-easy-tools"}}'
  CONTAINER_RUNTIME: '{{.CONTAINER_RUNTIME | default "podman"}}'

  # CMX VM configuration
  CMX_VM_NAME: '{{.CMX_VM_NAME | default (printf "%s-cmx-vm" (or (env "USER") "dev"))}}'
  CMX_VM_DISTRIBUTION: '{{.CMX_VM_DISTRIBUTION | default "ubuntu"}}'
  CMX_VM_VERSION: '{{.CMX_VM_VERSION | default "24.04"}}'
  CMX_VM_INSTANCE_TYPE: '{{.CMX_VM_INSTANCE_TYPE | default "r1.medium"}}'
  CMX_VM_DISK_SIZE: '{{.CMX_VM_DISK_SIZE | default "100"}}'
  CMX_VM_TTL: '{{.CMX_VM_TTL | default "1h"}}'
  CMX_VM_USER: '{{.CMX_VS_USER}}'
  CMX_VM_PUBLIC_KEY: '{{.CMX_VM_PUBLIC_KEY}}'

silent: true

tasks:
  default:
    desc: Show available tasks
    cmds:
      - task --list

  debug-env:
    desc: Debug environment variables and task variables
    cmds:
      - |
        echo "DEBUG: Raw REPLICATED_APP env var=$REPLICATED_APP"
        echo "DEBUG: Resolved APP_SLUG={{.APP_SLUG}}"
        echo "DEBUG: REPLICATED_API_TOKEN first 10 chars=${REPLICATED_API_TOKEN:0:10}..."
        echo "DEBUG: All task vars - APP_SLUG={{.APP_SLUG}} RELEASE_CHANNEL={{.RELEASE_CHANNEL}}"

  cluster-create:
    desc: Create a test cluster using Replicated Compatibility Matrix (use EMBEDDED=true for embedded clusters)
    run: once
    vars:
      EMBEDDED: '{{.EMBEDDED | default "false"}}'
      TIMEOUT: '{{if eq .EMBEDDED "true"}}420{{else}}300{{end}}'
      TTL: '{{.TTL | default "4h"}}'
      # Normalize cluster name by replacing common git branch delimiters with hyphens
      # This matches how cluster slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CLUSTER_NAME:
        sh: echo "{{.CLUSTER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    status:
      - |
        # Check if cluster exists and output info if it does
        CLUSTER_INFO=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.NORMALIZED_CLUSTER_NAME}}")')
        if [ -n "$CLUSTER_INFO" ]; then
          echo "Found existing cluster {{.NORMALIZED_CLUSTER_NAME}}:"
          echo "$CLUSTER_INFO" | jq -r '"  ID: " + .id + "\n  Status: " + .status + "\n  Distribution: " + .distribution + "\n  Created: " + .created_at + "\n  Expires: " + .expires_at'
          exit 0
        fi
        exit 1
    cmds:
      - |
        echo "Creating new cluster {{.NORMALIZED_CLUSTER_NAME}}..."
        if [ "{{.EMBEDDED}}" = "true" ]; then
          echo "Creating embedded cluster {{.NORMALIZED_CLUSTER_NAME}} with license ID {{.REPLICATED_LICENSE_ID}}..."
          replicated cluster create --distribution embedded-cluster --name {{.NORMALIZED_CLUSTER_NAME}} --license-id {{.REPLICATED_LICENSE_ID}} --ttl {{.TTL}}
        else
          echo "Creating cluster {{.NORMALIZED_CLUSTER_NAME}} with distribution {{.DISTRIBUTION}}..."
          replicated cluster create --name {{.NORMALIZED_CLUSTER_NAME}} --distribution {{.DISTRIBUTION}} --version {{.K8S_VERSION}} --disk {{.DISK_SIZE}} --instance-type {{.INSTANCE_TYPE}} --ttl {{.TTL}}
        fi
      - task: utils:wait-for-cluster
        vars:
          TIMEOUT: "{{.TIMEOUT}}"
          CLUSTER_NAME: "{{.NORMALIZED_CLUSTER_NAME}}"

  cluster-list:
    desc: List the cluster
    vars:
      # Normalize cluster name by replacing common git branch delimiters with hyphens
      # This matches how cluster slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CLUSTER_NAME:
        sh: echo "{{.CLUSTER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    cmds:
      - |
        CLUSTER_ID=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.NORMALIZED_CLUSTER_NAME}}") | .id')
        EXPIRES=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.NORMALIZED_CLUSTER_NAME}}") | .expires_at')
        echo "{{.NORMALIZED_CLUSTER_NAME}} Cluster ID: ($CLUSTER_ID) Expires: ($EXPIRES)"

  test:
    desc: Run a basic test suite
    cmds:
      - echo "Running basic tests..."
      - echo "This is a placeholder for actual tests"
      - sleep 5
      - echo "Tests completed!"

  verify-kubeconfig:
    desc: Verify kubeconfig
    run: once
    vars:
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .CLUSTER_NAME)}}'
    cmds:
      - |
        if [ -f {{.KUBECONFIG_FILE}} ]; then
          echo "Getting Cluster ID From Replicated Cluster list"
          CLUSTER_ID=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.CLUSTER_NAME}}") | .id')
          echo "Getting Cluster ID From Kubeconfig"
          CLUSTER_ID_KUBECONFIG=$(grep "current-context:" {{.KUBECONFIG_FILE}} | cut -d'-' -f3)
          if [ "$CLUSTER_ID" != "$CLUSTER_ID_KUBECONFIG" ]; then
            echo "{{.CLUSTER_NAME}} Cluster ID between Replicated ($CLUSTER_ID) and Kubeconfig ($CLUSTER_ID_KUBECONFIG) mismatch"
            echo "Removing old kubeconfig file"
            rm -f {{.KUBECONFIG_FILE}}
          fi
        fi

  setup-kubeconfig:
    desc: Get kubeconfig and prepare cluster for application deployment
    run: once
    vars:
      CLUSTER_NAME: '{{.CLUSTER_NAME | default .CLUSTER_NAME}}'
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .CLUSTER_NAME)}}'
    cmds:
      - task: utils:get-kubeconfig
        vars:
          CLUSTER_NAME: '{{.CLUSTER_NAME}}'
          KUBECONFIG_FILE: '{{.KUBECONFIG_FILE}}'
      - task: utils:remove-k3s-traefik
        vars:
          CLUSTER_NAME: '{{.CLUSTER_NAME}}'
          KUBECONFIG_FILE: '{{.KUBECONFIG_FILE}}'
      - echo "{{.KUBECONFIG_FILE}}"
    status:
      - |
        # Check if kubeconfig exists
        test -f {{.KUBECONFIG_FILE}} && \
        # For k3s, also check if traefik is removed
        if [ "{{.DISTRIBUTION}}" = "k3s" ]; then
          KUBECONFIG={{.KUBECONFIG_FILE}} helm list -n kube-system -o json | \
            jq -e 'map(select(.name == "traefik" or .name == "traefik-crd")) | length == 0' >/dev/null
        else
          true
        fi
    deps:
      - cluster-create
      - verify-kubeconfig

  helm-repo-add:
    desc: Add all HTTP/HTTPS Helm repositories found in Chart.yaml files
    silent: false
    run: once
    cmds:
      - echo "Adding Helm repositories from Chart.yaml files..."
      - |
        # Find all Chart.yaml files and extract HTTP/HTTPS repositories
        for chart_file in $(find charts/ -maxdepth 2 -name "Chart.yaml"); do
          echo "Processing $chart_file"

          # Extract repository URLs that start with http:// or https://
          yq eval '.dependencies[]?.repository' "$chart_file" 2>/dev/null | grep -E '^https?://' | while read -r repo_url; do
            if [ -n "$repo_url" ]; then
              # Generate a repository name from the URL
              repo_name=$(echo "$repo_url" | sed 's|https\?://||' | sed 's|[./]|-|g' | sed 's|-*$||')

              echo "Adding repository: $repo_name -> $repo_url"
              helm repo add "$repo_name" "$repo_url" || echo "Repository $repo_name may already exist"
            fi
          done
        done
      - echo "Updating Helm repository index..."
      - helm repo update
      - echo "All Helm repositories added and updated!"

  dependencies-update:
    desc: Update Helm dependencies for all charts
    run: once
    cmds:
      - echo "Ensure Helm credentials are cleared..."
      - helm registry logout registry.replicated.com || true
      - echo "Updating Helm dependencies for all charts..."
      - |
        # Find all charts and update their dependencies
        for chart_dir in $(find charts/ -maxdepth 2 -name "Chart.yaml" | xargs dirname); do
          echo "Updating dependency $chart_dir"
          helm dependency update --skip-refresh "$chart_dir"
        done
      - echo "All dependencies updated!"
    deps:
      - helm-repo-add

  cluster-ports-expose:
    desc: Expose configured ports for a cluster and capture exposed URLs
    run: once
    status:
      - |
        CLUSTER_ID=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.CLUSTER_NAME}}") | .id')
        if [ -z "$CLUSTER_ID" ]; then
          exit 1
        fi

        # Check if all ports are already exposed
        expected_count={{len .EXPOSE_PORTS}}
        port_checks=""
        {{range $i, $port := .EXPOSE_PORTS}}
        port_checks="${port_checks}(.upstream_port == {{$port.port}} and .exposed_ports[0].protocol == \"{{$port.protocol}}\") or "
        {{end}}
        # Remove trailing "or "
        port_checks="${port_checks% or }"

        PORT_COUNT=$(replicated cluster port ls $CLUSTER_ID --output json | jq -r ".[] | select($port_checks) | .upstream_port" | wc -l | tr -d ' ')
        [ "$PORT_COUNT" -eq "$expected_count" ]
    cmds:
      - task: utils:port-operations
        vars:
          OPERATION: "expose"
    deps:
      - cluster-create

  helm-preflight:
    desc: Run preflight checks on Helm charts using preflight CLI (use DRY_RUN=true for dry-run)
    vars:
      DRY_RUN: '{{.DRY_RUN | default "false"}}'
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .CLUSTER_NAME)}}'
    cmds:
      - |
        PREFLIGHT_FLAGS=""
        if [ "{{.DRY_RUN}}" = "true" ]; then
          PREFLIGHT_FLAGS="--dry-run"
        fi

        for chart_dir in $(find charts/ -maxdepth 2 -name "Chart.yaml" | xargs dirname); do
          echo "Running preflight on $chart_dir"
          KUBECONFIG={{.KUBECONFIG_FILE}} helm template $chart_dir | KUBECONFIG={{.KUBECONFIG_FILE}} kubectl preflight - $PREFLIGHT_FLAGS
        done
    deps:
      - setup-kubeconfig

  helm-install:
    desc: Install all charts using helmfile
    vars:
      HELM_ENV: '{{.HELM_ENV | default "default"}}'
      CLUSTER_NAME: '{{.CLUSTER_NAME}}'
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .CLUSTER_NAME)}}'
      REPLICATED_LICENSE_ID: '{{.REPLICATED_LICENSE_ID}}'
      CHANNEL: '{{.CHANNEL}}'
    cmds:
      - |
        echo "Installing all charts via helmfile"
        echo "DEBUG: APP_SLUG={{.APP_SLUG}}"
        echo "DEBUG: REPLICATED_APP env var=$REPLICATED_APP"
        echo "DEBUG: HELM_ENV={{.HELM_ENV}}"
      - |
        # Get cluster ID
        CLUSTER_ID=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.CLUSTER_NAME}}") | .id')
        if [ -z "$CLUSTER_ID" ]; then
          echo "Error: Could not find cluster with name {{.CLUSTER_NAME}}"
          exit 1
        fi

        # Get exposed URLs
        ENV_VARS=$(task utils:port-operations OPERATION=getenv CLUSTER_NAME={{.CLUSTER_NAME}} )

        # Deploy with helmfile
        echo "Using $ENV_VARS"
        if [ "{{.HELM_ENV}}" = "replicated" ]; then
          eval "KUBECONFIG='{{.KUBECONFIG_FILE}}' HELMFILE_ENVIRONMENT='{{.HELM_ENV}}' REPLICATED_APP='{{.APP_SLUG}}' REPLICATED_LICENSE_ID='{{.REPLICATED_LICENSE_ID}}' CHANNEL='{{.CHANNEL}}' $ENV_VARS helmfile sync --wait"
        else
          eval "KUBECONFIG='{{.KUBECONFIG_FILE}}' HELMFILE_ENVIRONMENT='{{.HELM_ENV}}' REPLICATED_APP='{{.APP_SLUG}}' $ENV_VARS helmfile sync --wait"
        fi
      - echo "All charts installed!"
    deps:
      - setup-kubeconfig
      - cluster-ports-expose

  helm-uninstall:
    desc: Uninstall all charts using helm uninstall
    silent: false
    vars:
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .CLUSTER_NAME)}}'
    cmds:
      - echo "Uninstalling all charts via helm"
      - |
        # Get cluster ID
        CLUSTER_ID=$(replicated cluster ls --output json | jq -r '.[] | select(.name == "{{.CLUSTER_NAME}}") | .id')
        if [ -z "$CLUSTER_ID" ]; then
          echo "Error: Could not find cluster with name {{.CLUSTER_NAME}}"
          exit 1
        fi

        # Get the list of installed releases and uninstall them
        KUBECONFIG={{.KUBECONFIG_FILE}} helm list --all-namespaces -o json | jq -r '.[] | .name + " -n " + .namespace' | while read release; do
          echo "Uninstalling helm release: $release"
          KUBECONFIG={{.KUBECONFIG_FILE}} helm uninstall $release
        done
      - echo "All charts uninstalled!"
    deps:
      - setup-kubeconfig

  cluster-delete:
    desc: Delete all test clusters with matching name and clean up kubeconfig
    silent: false
    vars:
      # Normalize cluster name by replacing common git branch delimiters with hyphens
      # This matches how cluster slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CLUSTER_NAME:
        sh: echo "{{.CLUSTER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .NORMALIZED_CLUSTER_NAME)}}'
    cmds:
      - echo "Deleting clusters named {{.NORMALIZED_CLUSTER_NAME}}..."
      - |
        CLUSTER_IDS=$(replicated cluster ls | grep "{{.NORMALIZED_CLUSTER_NAME}}" | awk '{print $1}')
        if [ -z "$CLUSTER_IDS" ]; then
          echo "No clusters found with name {{.NORMALIZED_CLUSTER_NAME}}"
          exit 0
        fi

        for id in $CLUSTER_IDS; do
          echo "Deleting cluster ID: $id"
          replicated cluster rm "$id"
        done
      - |
        # Clean up kubeconfig file
        if [ -f "{{.KUBECONFIG_FILE}}" ]; then
          echo "Removing kubeconfig file {{.KUBECONFIG_FILE}}"
          rm "{{.KUBECONFIG_FILE}}"
        fi
      - echo "All matching clusters deleted and kubeconfig cleaned up!"

  release-prepare:
    desc: Prepare release files by copying replicated YAML files and packaging Helm charts
    cmds:
      - echo "Preparing release files..."
      - rm -rf ./release
      - mkdir -p ./release

      # Copy all non-config.yaml files
      - echo "Copying non-config YAML files to release folder..."
      - find . -path './charts/*/replicated/*.yaml' -exec cp {} ./release/ \;
      - find ./replicated -name '*.yaml' -not -name 'config.yaml' -exec cp {} ./release/ \; 2>/dev/null || true

      # extract namespaces from helmChart files
      - echo "Extracting namespaces from helmChart files..."
      - yq ea '[.spec.namespace] | unique' ./charts/*/replicated/helmChart-*.yaml | yq '.spec.additionalNamespaces *= load("/dev/stdin") | .spec.additionalNamespaces += "*" ' replicated/application.yaml > release/application.yaml.new
      - cat release/application.yaml.new
      - mv release/application.yaml.new release/application.yaml

      # set helmChart versions from associated helm Chart.yaml
      - echo "Setting helmChart versions..."
      - |
        # Find all replicated directories and update helmChart files in one loop
        find ./charts -maxdepth 2 -mindepth 2 -type d -name replicated | while read chartDir; do
          echo $chartDir
          parent=$(basename $(dirname $chartDir))
          helmChartName="helmChart-$parent.yaml"
          export version=$(yq -r '.version' $chartDir/../Chart.yaml )
          yq '.spec.chart.chartVersion = strenv(version) | .spec.chart.chartVersion style="single"' $chartDir/$helmChartName | tee release/$helmChartName
        done

      # Merge config.yaml files
      - echo "Merging config.yaml files..."
      - |
        # Start with an empty config file
        echo "{}" > ./release/config.yaml

        # Merge all app config.yaml files first (excluding root replicated)
        for config_file in $(find . -path '*/replicated/config.yaml' | grep -v "^./replicated/"); do
          echo "Merging $config_file..."
          yq eval-all '. as $item ireduce ({}; . * $item)' ./release/config.yaml "$config_file" > ./release/config.yaml.new
          mv ./release/config.yaml.new ./release/config.yaml
        done

        # Merge root config.yaml last
        if [ -f "./replicated/config.yaml" ]; then
          echo "Merging root config.yaml last..."
          yq eval-all '. as $item ireduce ({}; . * $item)' ./release/config.yaml "./replicated/config.yaml" > ./release/config.yaml.new
          mv ./release/config.yaml.new ./release/config.yaml
        fi

      # Package Helm charts
      - echo "Packaging Helm charts..."
      - |
        # Find top-level directories containing Chart.yaml files, excluding the templates chart
        for chart_dir in $(find charts/ -maxdepth 2 -name "Chart.yaml" | grep -v "charts/templates" | xargs dirname); do
          echo "Packaging chart: $chart_dir"
          # Navigate to chart directory, package it, and move the resulting .tgz to release folder
          (cd "$chart_dir" && helm package . && mv *.tgz ../../release/)
        done

      - echo "Release files prepared in ./release/ directory"
    deps:
      - dependencies-update

  release-create:
    desc: Create and promote a release using the Replicated CLI (supports both channel names and IDs)
    run: once
    vars:
      RELEASE_CHANNEL: '{{.RELEASE_CHANNEL | default "Unstable"}}'
      RELEASE_CHANNEL_ID: '{{.RELEASE_CHANNEL_ID}}'
      RELEASE_VERSION: '{{.RELEASE_VERSION | default "0.0.1"}}'
      RELEASE_NOTES: '{{.RELEASE_NOTES | default "Release created via task release-create"}}'
      # Use channel ID if provided, otherwise fall back to channel name
      CHANNEL_TARGET: '{{if .RELEASE_CHANNEL_ID}}{{.RELEASE_CHANNEL_ID}}{{else}}{{.RELEASE_CHANNEL}}{{end}}'
    requires:
      vars: [APP_SLUG, RELEASE_VERSION]
    cmds:
      - |
        if [ -n "{{.RELEASE_CHANNEL_ID}}" ]; then
          echo "Creating and promoting release for {{.APP_SLUG}} to channel ID {{.RELEASE_CHANNEL_ID}}..."
        else
          echo "Creating and promoting release for {{.APP_SLUG}} to channel {{.RELEASE_CHANNEL}}..."
        fi
      - |
        # Create and promote the release in one step
        echo "Creating release from files in ./release directory..."
        replicated release create --app {{.APP_SLUG}} --yaml-dir ./release --release-notes "{{.RELEASE_NOTES}}" --promote {{.CHANNEL_TARGET}} --version {{.RELEASE_VERSION}}
        if [ -n "{{.RELEASE_CHANNEL_ID}}" ]; then
          echo "Release version {{.RELEASE_VERSION}} created and promoted to channel ID {{.RELEASE_CHANNEL_ID}}"
        else
          echo "Release version {{.RELEASE_VERSION}} created and promoted to channel {{.RELEASE_CHANNEL}}"
        fi
    deps:
      - release-prepare

  customer-create:
    desc: Create a new customer or get existing customer with matching name and return their ID
    run: once
    vars:
      CUSTOMER_NAME: '{{.CUSTOMER_NAME | default "test-customer"}}'
      CUSTOMER_EMAIL: '{{.CUSTOMER_EMAIL | default "test@example.com"}}'
      RELEASE_CHANNEL: '{{.RELEASE_CHANNEL | default "Unstable"}}'
      RELEASE_CHANNEL_ID: '{{.RELEASE_CHANNEL_ID}}'
      LICENSE_TYPE: '{{.LICENSE_TYPE | default "dev"}}'
      EXPIRES_IN: '{{.EXPIRES_IN | default ""}}'
      # Normalize customer name by replacing common git branch delimiters with hyphens
      # This matches how customer slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CUSTOMER_NAME:
        sh: echo "{{.CUSTOMER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    requires:
      vars: [APP_SLUG]
    cmds:
      - |
        # First check if customer already exists
        echo "Looking for existing customer {{.NORMALIZED_CUSTOMER_NAME}} for app {{.APP_SLUG}}..."
        EXISTING_CUSTOMER=$(replicated customer ls --app {{.APP_SLUG}} --output json | jq -r 'if type == "array" then .[] | select(.name=="{{.NORMALIZED_CUSTOMER_NAME}}") | .id else empty end' 2>/dev/null | head -1)

        if [ -n "$EXISTING_CUSTOMER" ] && [ "$EXISTING_CUSTOMER" != "null" ]; then
          echo "Found existing customer {{.NORMALIZED_CUSTOMER_NAME}} with ID: $EXISTING_CUSTOMER"
          echo "$EXISTING_CUSTOMER"
          exit 0
        fi

        # No existing customer found, create a new one
        echo "Creating new customer {{.NORMALIZED_CUSTOMER_NAME}} for app {{.APP_SLUG}}..."

        # Determine which channel parameter to use (--channel accepts both names and IDs)
        if [ -n "{{.RELEASE_CHANNEL_ID}}" ]; then
          CHANNEL_PARAM="--channel {{.RELEASE_CHANNEL_ID}}"
          echo "Using channel ID: {{.RELEASE_CHANNEL_ID}}"
        else
          CHANNEL_PARAM="--channel {{.RELEASE_CHANNEL}}"
          echo "Using channel name: {{.RELEASE_CHANNEL}}"
        fi

        # Build the command with optional expiration
        CMD="replicated customer create \
          --app {{.APP_SLUG}} \
          --name {{.NORMALIZED_CUSTOMER_NAME}} \
          --email {{.CUSTOMER_EMAIL}} \
          $CHANNEL_PARAM \
          --type {{.LICENSE_TYPE}} \
          --output json"

        # Add expiration if specified
        if [ -n "{{.EXPIRES_IN}}" ]; then
          CMD="$CMD --expires-in {{.EXPIRES_IN}}"
        fi

        # Create the customer and capture the output
        CUSTOMER_JSON=$(eval $CMD)

        # Extract and output just the customer ID
        echo "$CUSTOMER_JSON" | jq -r '.id'

  gcp-vm-create:
    desc: Create a simple GCP VM instance
    vars:
      GCP_MACHINE_TYPE: '{{.GCP_MACHINE_TYPE | default "e2-standard-2"}}'
      GCP_DISK_SIZE: '{{.GCP_DISK_SIZE | default "100"}}'
      GCP_DISK_TYPE: '{{.GCP_DISK_TYPE | default "pd-standard"}}'
      GCP_IMAGE_FAMILY: '{{.GCP_IMAGE_FAMILY | default "ubuntu-2204-lts"}}'
      GCP_IMAGE_PROJECT: '{{.GCP_IMAGE_PROJECT | default "ubuntu-os-cloud"}}'
    status:
      - gcloud compute instances describe {{.VM_NAME}} --project={{.GCP_PROJECT}} --zone={{.GCP_ZONE}} &>/dev/null
    cmds:
      - task: utils:gcp-operations
        vars:
          OPERATION: "create"
          GCP_MACHINE_TYPE: '{{.GCP_MACHINE_TYPE}}'
          GCP_DISK_SIZE: '{{.GCP_DISK_SIZE}}'
          GCP_DISK_TYPE: '{{.GCP_DISK_TYPE}}'
          GCP_IMAGE_FAMILY: '{{.GCP_IMAGE_FAMILY}}'
          GCP_IMAGE_PROJECT: '{{.GCP_IMAGE_PROJECT}}'

  gcp-vm-delete:
    desc: Delete the GCP VM instance for K8s and VPN
    status:
      - "! gcloud compute instances describe {{.VM_NAME}} --project={{.GCP_PROJECT}} --zone={{.GCP_ZONE}} &>/dev/null"
    cmds:
      - task: utils:gcp-operations
        vars:
          OPERATION: "delete"
          GCP_PROJECT: '{{.GCP_PROJECT}}'
          GCP_ZONE: '{{.GCP_ZONE}}'
          VM_NAME: '{{.VM_NAME}}'

  embedded-cluster-setup:
    desc: Setup Replicated embedded cluster on the GCP VM
    vars:
      RELEASE_CHANNEL: '{{.RELEASE_CHANNEL | default "Unstable"}}'
      AUTH_TOKEN: '{{.AUTH_TOKEN | default "2usDXzovcJNcpn54yS5tFQVNvCq"}}'
    deps:
      - gcp-vm-create
    status:
      - |
        # Check if the application tarball has already been downloaded and extracted
        gcloud compute ssh {{.VM_NAME}} --project={{.GCP_PROJECT}} --zone={{.GCP_ZONE}} --command="test -d ./{{.APP_SLUG}}" &>/dev/null
    cmds:
      - task: utils:gcp-operations
        vars:
          OPERATION: "setup-embedded"
          APP_SLUG: '{{.APP_SLUG}}'
          RELEASE_CHANNEL: '{{.RELEASE_CHANNEL}}'
          AUTH_TOKEN: '{{.REPLICATED_LICENSE_ID}}'
          GCP_PROJECT: '{{.GCP_PROJECT}}'
          GCP_ZONE: '{{.GCP_ZONE}}'
          VM_NAME: '{{.VM_NAME}}'

  customer-ls:
    desc: List customers for the application
    vars:
      OUTPUT_FORMAT: '{{.OUTPUT_FORMAT | default "table"}}'
    requires:
      vars: [APP_SLUG]
    cmds:
      - echo "Listing customers for app {{.APP_SLUG}}..."
      - replicated customer ls --app {{.APP_SLUG}} --output {{.OUTPUT_FORMAT}}

  customer-delete:
    desc: Archive a customer by ID
    vars:
      CUSTOMER_ID: '{{.CUSTOMER_ID}}'
    requires:
      vars: [APP_SLUG, CUSTOMER_ID]
    cmds:
      - echo "Archiving customer with ID {{.CUSTOMER_ID}} from app {{.APP_SLUG}}..."
      - |
        # Verify customer exists before attempting to archive
        CUSTOMER_EXISTS=$(replicated customer ls --app {{.APP_SLUG}} --output json | jq -r '.[] | select(.id=="{{.CUSTOMER_ID}}") | .id')
        if [ -z "$CUSTOMER_EXISTS" ]; then
          echo "Error: Customer with ID {{.CUSTOMER_ID}} not found for app {{.APP_SLUG}}"
          exit 1
        fi

        # Get customer name for confirmation message
        CUSTOMER_NAME=$(replicated customer ls --app {{.APP_SLUG}} --output json | jq -r '.[] | select(.id=="{{.CUSTOMER_ID}}") | .name')

        # Archive the customer
        replicated customer archive {{.CUSTOMER_ID}} --app {{.APP_SLUG}}

        # Confirm archiving
        echo "Customer '$CUSTOMER_NAME' (ID: {{.CUSTOMER_ID}}) successfully archived"

  channel-create:
    desc: Create a Replicated release channel and return its ID
    silent: false
    vars:
      RELEASE_CHANNEL: '{{.RELEASE_CHANNEL}}'
      # Normalize channel name by replacing common git branch delimiters with hyphens
      # This matches how channel slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_RELEASE_CHANNEL:
        sh: echo "{{.RELEASE_CHANNEL}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    requires:
      vars: [APP_SLUG, RELEASE_CHANNEL]
    cmds:
      - echo "Creating channel {{.NORMALIZED_RELEASE_CHANNEL}} for app {{.APP_SLUG}}..."
      - |
        # Check if channel already exists
        echo "Debug: Checking existing channels for app {{.APP_SLUG}}..."
        CHANNEL_LIST_RESPONSE=$(replicated channel ls --app {{.APP_SLUG}} --output json)
        echo "Debug: Channel list response: $CHANNEL_LIST_RESPONSE"
        EXISTING_CHANNEL_ID=$(echo "$CHANNEL_LIST_RESPONSE" | jq -r 'if type == "array" then .[] | select(.name=="{{.NORMALIZED_RELEASE_CHANNEL}}") | .id else empty end' 2>/dev/null | head -1)

        if [ -n "$EXISTING_CHANNEL_ID" ] && [ "$EXISTING_CHANNEL_ID" != "null" ]; then
          echo "Channel {{.NORMALIZED_RELEASE_CHANNEL}} already exists for app {{.APP_SLUG}} with ID: $EXISTING_CHANNEL_ID"
          echo "$EXISTING_CHANNEL_ID"
          exit 0
        fi

        # Create the channel and capture its ID
        CHANNEL_OUTPUT=$(replicated channel create --app {{.APP_SLUG}} --name {{.NORMALIZED_RELEASE_CHANNEL}} --output json)
        CHANNEL_ID=$(echo "$CHANNEL_OUTPUT" | jq -r '.id')
        echo "Channel {{.NORMALIZED_RELEASE_CHANNEL}} created successfully with ID: $CHANNEL_ID"
        echo "$CHANNEL_ID"

  channel-delete:
    desc: Archive a Replicated release channel by ID
    silent: false
    vars:
      RELEASE_CHANNEL_ID: '{{.RELEASE_CHANNEL_ID}}'
    requires:
      vars: [APP_SLUG, RELEASE_CHANNEL_ID]
    cmds:
      - echo "Archiving channel ID {{.RELEASE_CHANNEL_ID}} for app {{.APP_SLUG}}..."
      - |
        # Get channel name for logging
        CHANNEL_NAME=$(replicated channel ls --app {{.APP_SLUG}} --output json | jq -r 'if type == "array" then .[] | select(.id=="{{.RELEASE_CHANNEL_ID}}") | .name else empty end' 2>/dev/null | head -1)

        if [ -z "$CHANNEL_NAME" ] || [ "$CHANNEL_NAME" = "null" ]; then
          echo "Error: Channel ID {{.RELEASE_CHANNEL_ID}} not found for app {{.APP_SLUG}}"
          exit 1
        fi

        # Archive the channel
        replicated channel archive --app {{.APP_SLUG}} {{.RELEASE_CHANNEL_ID}}
        echo "Channel $CHANNEL_NAME (ID: {{.RELEASE_CHANNEL_ID}}) archived successfully"

  chart-lint-all:
    desc: Lint all Helm charts in the project
    run: once
    cmds:
      - echo "Linting all Helm charts..."
      - |
        # Find all charts and lint them
        for chart_dir in $(find charts/ -maxdepth 2 -name "Chart.yaml" | xargs dirname); do
          echo "Linting chart: $chart_dir"
          helm lint "$chart_dir"
        done
      - echo "All charts linted successfully!"
    deps:
      - dependencies-update

  chart-template-all:
    desc: Template all Helm charts to validate syntax
    run: once
    cmds:
      - echo "Templating all Helm charts..."
      - |
        # Find all charts and template them
        for chart_dir in $(find charts/ -maxdepth 2 -name "Chart.yaml" | xargs dirname); do
          echo "Templating chart: $chart_dir"
          helm template test-release "$chart_dir" --dry-run >/dev/null
        done
      - echo "All charts templated successfully!"
    deps:
      - dependencies-update

  chart-validate:
    desc: Validate all Helm charts (lint + template + helmfile)
    cmds:
      - task: chart-lint-all
      - task: chart-template-all
      - echo "Validating helmfile template..."
      - |
        if [ -f "helmfile.yaml.gotmpl" ]; then
          # Set required environment variables for helmfile validation
          export REPLICATED_APP="test-app"
          export CHANNEL="test-channel"
          export REPLICATED_LICENSE_ID="test-license"
          export TF_EXPOSED_URL="test.example.com"
          export HELMFILE_ENVIRONMENT="default"

          echo "Building helmfile template..."
          helmfile build >/dev/null
          echo "Helmfile template validation successful!"
        else
          echo "No helmfile.yaml.gotmpl found, skipping helmfile validation"
        fi

  chart-package-all:
    desc: Package all Helm charts for distribution
    cmds:
      - echo "Packaging all Helm charts..."
      - task: dependencies-update
      - task: release-prepare
      - echo "All charts packaged successfully!"

  clean:
    desc: Remove temporary Helm directories, chart dependencies, and release folder
    cmds:
      - echo "Cleaning temporary directories and dependencies..."
      - |
        # Remove the release directory
        if [ -d "./release" ]; then
          echo "Removing release directory..."
          rm -rf ./release
        fi

        # Find and remove tmpcharts-* directories in charts/
        echo "Removing temporary chart directories..."
        find charts/ -type d -name "tmpcharts-*" -print
        find charts/ -type d -name "tmpcharts-*" -exec rm -rf {} \; 2>/dev/null || true

        # Clean up chart dependencies (.tgz files) in charts/*/charts/
        echo "Removing chart dependencies..."
        find charts/ -path "*/charts/*.tgz" -type f -print
        find charts/ -path "*/charts/*.tgz" -type f -delete

        # Clean up any tmpcharts directories in subdirectories
        echo "Cleaning up any remaining tmpcharts directories..."
        find . -type d -name "tmpcharts-*" -print
        find . -type d -name "tmpcharts-*" -exec rm -rf {} \; 2>/dev/null || true
      - echo "Cleaning complete!"

  pr-validation-cycle:
    desc: Complete PR validation workflow (validate charts, create release, test deployment)
    vars:
      BRANCH_NAME: '{{.BRANCH_NAME | default "pr-test"}}'
      CHANNEL_NAME: '{{.CHANNEL_NAME | default .BRANCH_NAME}}'
      # Normalize names by replacing common git branch delimiters with hyphens
      # This matches how slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_BRANCH_NAME:
        sh: echo "{{.BRANCH_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      NORMALIZED_CHANNEL_NAME:
        sh: echo "{{.CHANNEL_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      CHANNEL_ID:
        sh: task channel-create RELEASE_CHANNEL={{.NORMALIZED_CHANNEL_NAME}}
    requires:
      vars: [BRANCH_NAME]
    cmds:
      - echo "Starting PR validation cycle for branch {{.NORMALIZED_BRANCH_NAME}}"
      - echo "Step 1 - Validating charts..."
      - task: chart-validate
      - echo "Step 2 - Building and creating release..."
      - task: release-create
        vars:
          RELEASE_CHANNEL: "{{.NORMALIZED_CHANNEL_NAME}}"
      - echo "Step 3 - Testing deployment..."
      - task: customer-create
        vars:
          CUSTOMER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
          RELEASE_CHANNEL_ID: "{{.CHANNEL_ID}}"
      - task: cluster-create
        vars:
          CLUSTER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
      - task: setup-kubeconfig
        vars:
          CLUSTER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
      - task: cluster-ports-expose
        vars:
          CLUSTER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
      - task: customer-helm-install
        vars:
          CUSTOMER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
          CLUSTER_NAME: "{{.NORMALIZED_BRANCH_NAME}}"
          CHANNEL_ID: "{{.CHANNEL_ID}}"
          REPLICATED_LICENSE_ID:
            sh: task utils:get-customer-license CUSTOMER_NAME={{.NORMALIZED_BRANCH_NAME}}
      - task: test
      - echo "PR validation cycle completed successfully!"

  cleanup-pr-resources:
    desc: Cleanup PR-related resources (clusters, customers, channels)
    vars:
      BRANCH_NAME: '{{.BRANCH_NAME | default "pr-test"}}'
      CHANNEL_NAME: '{{.CHANNEL_NAME | default .BRANCH_NAME}}'
      # Normalize names by replacing common git branch delimiters with hyphens
      # This matches how slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_BRANCH_NAME:
        sh: echo "{{.BRANCH_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      NORMALIZED_CHANNEL_NAME:
        sh: echo "{{.CHANNEL_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    requires:
      vars: [BRANCH_NAME]
    cmds:
      - echo "Cleaning up PR resources for branch {{.NORMALIZED_BRANCH_NAME}}"
      - echo "Deleting cluster..."
      - |
        task cluster-delete CLUSTER_NAME="{{.NORMALIZED_BRANCH_NAME}}" || echo "Cluster deletion failed or cluster not found"
      - echo "Archiving customer..."
      - |
        CUSTOMER_ID=$(replicated customer ls --app {{.APP_SLUG}} --output json | jq -r 'if type == "array" then .[] | select(.name=="{{.NORMALIZED_BRANCH_NAME}}") | .id else empty end' 2>/dev/null | head -1)
        if [ -n "$CUSTOMER_ID" ] && [ "$CUSTOMER_ID" != "null" ]; then
          task customer-delete CUSTOMER_ID="$CUSTOMER_ID" || echo "Customer deletion failed"
        else
          echo "No customer found with name {{.NORMALIZED_BRANCH_NAME}}"
        fi
      - echo "Archiving channel..."
      - |
        # Get channel ID and delete it
        CHANNEL_ID=$(replicated channel ls --app {{.APP_SLUG}} --output json | jq -r 'if type == "array" then .[] | select(.name=="{{.NORMALIZED_CHANNEL_NAME}}") | .id else empty end' 2>/dev/null | head -1)
        if [ -n "$CHANNEL_ID" ] && [ "$CHANNEL_ID" != "null" ]; then
          task channel-delete RELEASE_CHANNEL_ID="$CHANNEL_ID" || echo "Channel deletion failed"
        else
          echo "No channel found with name {{.NORMALIZED_CHANNEL_NAME}}"
        fi

      - echo "PR resource cleanup completed!"

  full-test-cycle:
    desc: Create cluster, get kubeconfig, expose ports, update dependencies, deploy charts, test, and delete, and clean up build artifacts
    cmds:
      - task: cluster-create
      - task: setup-kubeconfig
      - task: cluster-ports-expose
      - task: dependencies-update
      - task: helm-preflight
      - task: helm-install
      - task: test
      - task: cluster-delete

  customer-helm-install:
    desc: Deploy charts using replicated environment with customer license and channel
    vars:
      CUSTOMER_NAME: '{{.CUSTOMER_NAME}}'
      CLUSTER_NAME: '{{.CLUSTER_NAME}}'
      REPLICATED_LICENSE_ID: '{{.REPLICATED_LICENSE_ID}}'
      CHANNEL_SLUG: '{{.CHANNEL_SLUG}}'
      CHANNEL_ID: '{{.CHANNEL_ID}}'
      # Normalize names by replacing common git branch delimiters with hyphens
      # This matches how slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CUSTOMER_NAME:
        sh: echo "{{.CUSTOMER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      NORMALIZED_CLUSTER_NAME:
        sh: echo "{{.CLUSTER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      NORMALIZED_CHANNEL_SLUG:
        sh: echo "{{.CHANNEL_SLUG}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      KUBECONFIG_FILE: '{{.KUBECONFIG_FILE | default (printf "./%s.kubeconfig" .NORMALIZED_CLUSTER_NAME)}}'
    requires:
      vars: [CUSTOMER_NAME, CLUSTER_NAME, REPLICATED_LICENSE_ID]
    cmds:
      - |
        echo "Deploying charts for customer {{.NORMALIZED_CUSTOMER_NAME}} using replicated environment..."
        echo "Cluster:{{.NORMALIZED_CLUSTER_NAME}}"
        echo "DEBUG: APP_SLUG={{.APP_SLUG}}"
        echo "DEBUG: REPLICATED_APP env var=$REPLICATED_APP"
      - |
        # Determine channel identifier to use and log it
        if [ -n "{{.CHANNEL_ID}}" ]; then
          echo "Channel ID:{{.CHANNEL_ID}}"
          CHANNEL_PARAM="{{.CHANNEL_ID}}"
        else
          echo "Channel Slug:{{.NORMALIZED_CHANNEL_SLUG}}"
          CHANNEL_PARAM="{{.NORMALIZED_CHANNEL_SLUG}}"
        fi
        echo "License ID:{{.REPLICATED_LICENSE_ID}}"
      - |
        # Get customer email for registry authentication
        echo "Getting customer email for registry authentication..."
        CUSTOMER_EMAIL=$(replicated customer inspect --customer $(replicated customer ls --app "{{.APP_SLUG}}" --output json | jq -r '.[] | select(.name == "{{.NORMALIZED_CUSTOMER_NAME}}") | .id') --app "{{.APP_SLUG}}" | grep "EMAIL:" | awk '{print $2}')
        echo "Customer email: $CUSTOMER_EMAIL"

        # Authenticate with Replicated registry using customer email and license ID
        echo "Authenticating with Replicated registry..."
        echo "{{.REPLICATED_LICENSE_ID}}" | helm registry login registry.replicated.com --username "$CUSTOMER_EMAIL" --password-stdin
      - |
        # Determine which channel parameter to use for helm install
        if [ -n "{{.CHANNEL_ID}}" ]; then
          CHANNEL_PARAM="{{.CHANNEL_ID}}"
        else
          CHANNEL_PARAM="{{.NORMALIZED_CHANNEL_SLUG}}"
        fi
        
        # Deploy using replicated environment with customer-specific settings
        task helm-install HELM_ENV=replicated REPLICATED_LICENSE_ID="{{.REPLICATED_LICENSE_ID}}" CHANNEL="$CHANNEL_PARAM" KUBECONFIG_FILE="{{.KUBECONFIG_FILE}}" CLUSTER_NAME="{{.NORMALIZED_CLUSTER_NAME}}"
      - echo "Customer helm install complete for {{.NORMALIZED_CUSTOMER_NAME}}"

  customer-full-test-cycle:
    desc: Complete customer workflow - create cluster, find customer, deploy using existing releases, test (no cleanup for CD)
    vars:
      CUSTOMER_NAME: '{{.CUSTOMER_NAME}}'
      CLUSTER_NAME: '{{.CLUSTER_NAME}}'
      # Normalize names by replacing common git branch delimiters with hyphens
      # This matches how slugs are represented in the Replicated Vendor Portal backend
      NORMALIZED_CUSTOMER_NAME:
        sh: echo "{{.CUSTOMER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
      NORMALIZED_CLUSTER_NAME:
        sh: echo "{{.CLUSTER_NAME}}" | tr '/' '-' | tr '_' '-' | tr '.' '-'
    requires:
      vars: [CUSTOMER_NAME, CLUSTER_NAME]
    cmds:
      - echo "Starting customer full test cycle..."
      - echo "Customer:{{.NORMALIZED_CUSTOMER_NAME}}"
      - echo "Cluster:{{.NORMALIZED_CLUSTER_NAME}}"

      # Setup cluster infrastructure
      - task: cluster-create
        vars:
          CLUSTER_NAME: '{{.NORMALIZED_CLUSTER_NAME}}'
      - task: setup-kubeconfig
        vars:
          CLUSTER_NAME: '{{.NORMALIZED_CLUSTER_NAME}}'
      - task: cluster-ports-expose
        vars:
          CLUSTER_NAME: '{{.NORMALIZED_CLUSTER_NAME}}'
      # - task: dependencies-update

      # Setup customer and get license (use existing releases)
      - echo "Creating/finding customer {{.NORMALIZED_CUSTOMER_NAME}}..."
      - task: customer-create
        vars:
          CUSTOMER_NAME: '{{.NORMALIZED_CUSTOMER_NAME}}'
      - echo "Getting license ID and channel for customer {{.NORMALIZED_CUSTOMER_NAME}}..."
      - task: customer-helm-install
        vars:
          CUSTOMER_NAME: '{{.NORMALIZED_CUSTOMER_NAME}}'
          CLUSTER_NAME: '{{.NORMALIZED_CLUSTER_NAME}}'
          REPLICATED_LICENSE_ID:
            sh: task utils:get-customer-license CUSTOMER_NAME={{.NORMALIZED_CUSTOMER_NAME}}
          CHANNEL_ID:
            sh: replicated customer ls --app {{.APP_SLUG}} --output json | jq -r '.[] | select(.name == "{{.NORMALIZED_CUSTOMER_NAME}}") | .channels[0].channelId'

      # Run tests
      - task: test

      - echo "Customer full test cycle complete! Environment left running for continuous deployment."
      - echo "Cluster:{{.NORMALIZED_CLUSTER_NAME}}"
      - echo "Customer:{{.NORMALIZED_CUSTOMER_NAME}}"

  cmx-vm-create:
    desc: Create a CMX VM instance using Replicated CLI
    run: once
    status:
      - |
        # Check if VM is running
        replicated vm ls | grep "{{.CMX_VM_NAME}}" | grep running
    cmds:
      - |
        echo "Creating CMX VM {{.CMX_VM_NAME}}..."
        replicated vm create --distribution {{.CMX_VM_DISTRIBUTION}} --version {{.CMX_VM_VERSION}} --instance-type {{.CMX_VM_INSTANCE_TYPE}} --disk {{.CMX_VM_DISK_SIZE}} --name {{.CMX_VM_NAME}} --ttl {{.CMX_VM_TTL}}

        echo "Waiting for VM to be running (timeout: 120s)..."
        for i in $(seq 1 60); do
          replicated vm ls | grep "{{.CMX_VM_NAME}}"
          if replicated vm ls | grep "{{.CMX_VM_NAME}}" | grep running; then
            echo "VM {{.CMX_VM_NAME}} is ready!"
            break
          fi
          if [ $i -eq 60 ]; then
            echo "Timeout reached after 120s. VM may still be initializing."
            exit 1
          fi
          sleep 2
        done

  cmx-vm-delete:
    desc: Delete a CMX VM instance
    cmds:
      - |
        echo "Deleting CMX VM {{.CMX_VM_NAME}}..."
        replicated vm rm {{.CMX_VM_NAME}}

  cmx-vm-install:
    desc: Download and install the app as Embedded Cluster on CMX VM
    requires:
      vars: [REPLICATED_LICENSE_ID]
    vars:
      CHANNEL: '{{.CHANNEL | default "Unstable"}}'
      SKIP_INSTALL: '{{.SKIP_INSTALL | default "false"}}'
      AIRGAP: '{{.AIRGAP | default "false"}}'
      ADMIN_CONSOLE_PASSWORD:
        sh: uuidgen | cut -c1-13
    deps:
      - cmx-vm-create
    status:
      - replicated vm port ls {{.CMX_VM_NAME}}
      - curl -sL $(replicated vm port ls {{.CMX_VM_NAME}} --output=json | jq -r ".[0].hostname")/healthz
    cmds:
      - |
        echo "Check if user is set..."
        if [ -z "{{.CMX_VM_USER}}" ]; then
          echo "CMX_VM_USER is not set. Set it to your Github handle. E.g. task cmx-vm-install CMX_VM_USER=nvanthao"
          exit 1
        fi

        echo "Check if license ID is set..."
        if [ -z "{{.REPLICATED_LICENSE_ID}}" ]; then
          echo "REPLICATED_LICENSE_ID is not set. Set it to your DEV Customer License ID. E.g. task cmx-vm-install REPLICATED_LICENSE_ID=1234567890"
          exit 1
        fi

        # Run airgap-build task if airgap mode is enabled
        if [ "{{.AIRGAP}}" = "true" ]; then
          echo "Airgap mode enabled, ensuring airgap build is ready..."
          task airgap-build
        fi

        echo "SSH into the VM and download the app binary..."
        SSH_BASE_CMD="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no"
        if [ -n "{{.CMX_VM_PUBLIC_KEY}}" ]; then
          SSH_BASE_CMD="$SSH_BASE_CMD -i {{.CMX_VM_PUBLIC_KEY}}"
        fi
        VM_SSH_CMD=$(replicated vm ls --output=json | jq -r ".[] | select(.name == \"{{.CMX_VM_NAME}}\") | \"$SSH_BASE_CMD -p \(.direct_ssh_port) {{.CMX_VM_USER}}@\(.direct_ssh_endpoint)\"")

        # Determine download URL based on airgap setting
        DOWNLOAD_URL="https://replicated.app/embedded/{{.APP_SLUG}}/{{.CHANNEL}}"
        if [ "{{.AIRGAP}}" = "true" ]; then
          DOWNLOAD_URL="${DOWNLOAD_URL}?airgap=true"
        fi

        echo "SSH base command: $SSH_BASE_CMD"
        $VM_SSH_CMD << EOF
        set -e
        echo 'Downloading {{.APP_SLUG}} installer...'
        curl -f '$DOWNLOAD_URL' -H 'Authorization: {{.REPLICATED_LICENSE_ID}}' -o {{.APP_SLUG}}-{{.CHANNEL}}.tgz

        echo 'Extracting installer...'
        tar -xvzf {{.APP_SLUG}}-{{.CHANNEL}}.tgz

        echo "Binary is available at ./{{.APP_SLUG}}"
        EOF

        if [ "{{.AIRGAP}}" = "true" ]; then
          echo "Updating network policy for airgap installation..."
          NETWORK_ID=$(replicated vm ls --output=json | jq -r ".[] | select(.name == \"{{.CMX_VM_NAME}}\") | .network_id")
          replicated network update policy $NETWORK_ID --policy airgap
        fi

        if [ "{{.SKIP_INSTALL}}" = "false" ]; then
          INSTALL_CMD="sudo ./{{.APP_SLUG}} install --license license.yaml --admin-console-password {{.ADMIN_CONSOLE_PASSWORD}} --yes"
          if [ "{{.AIRGAP}}" = "true" ]; then
            INSTALL_CMD="${INSTALL_CMD} --airgap-bundle {{.APP_SLUG}}.airgap"
          fi

          echo "Running installation command: $INSTALL_CMD"
        $VM_SSH_CMD << EOF
        $INSTALL_CMD
        EOF

          echo "Exposing port 30000 on the VM..."
          replicated vm port expose --port 30000 {{.CMX_VM_NAME}}

          echo "Visit above URL to access the Admin Console, password: {{.ADMIN_CONSOLE_PASSWORD}}"
        fi

  airgap-build:
    desc: Check and build airgap bundle for the latest release
    silent: true
    cmds:
      - |
        echo "Checking if airgap build is available for latest release in channel {{.RELEASE_CHANNEL}}..."

        # Get release list and extract app ID and channel ID
        RELEASE_DATA=$(replicated release ls -o json)
        APP_ID=$(echo "$RELEASE_DATA" | jq -r '.[0].appId')
        # Try to get channel ID from parameter first, fall back to channel name lookup
        if [ -n "{{.RELEASE_CHANNEL_ID}}" ]; then
          CHANNEL_ID="{{.RELEASE_CHANNEL_ID}}"
          echo "Using provided channel ID: $CHANNEL_ID"
        else
          CHANNEL_ID=$(echo "$RELEASE_DATA" | jq -r '.[0].activeChannels[] | select(.name == "{{.RELEASE_CHANNEL}}") | .id')
          echo "Looked up channel ID for {{.RELEASE_CHANNEL}}: $CHANNEL_ID"
        fi

        if [ -z "$APP_ID" ] || [ "$APP_ID" = "null" ]; then
          echo "Error: Could not retrieve app ID from latest releases"
          exit 1
        fi

        if [ -z "$CHANNEL_ID" ] || [ "$CHANNEL_ID" = "null" ]; then
          echo "Error: Could not find channel ID for channel {{.RELEASE_CHANNEL}}"
          exit 1
        fi

        echo "Found app ID: $APP_ID, channel ID: $CHANNEL_ID"

        # Get channel releases and check airgap build status
        CHANNEL_RELEASES=$(replicated api get "v3/app/$APP_ID/channel/$CHANNEL_ID/releases")
        AIRGAP_BUILD_STATUS=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBuildStatus // "none"')
        AIRGAP_BUILD_ERROR=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBuildError // "none"')
        AIRGAP_BUNDLE_IMAGES=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBundleImages // "none"')
        AIRGAP_LATEST_SEQUENCE=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].channelSequence')

        echo "Airgap build status: $AIRGAP_BUILD_STATUS"

        if [ "$AIRGAP_BUILD_STATUS" = "built" ]; then
          echo "Airgap is already built for sequence $AIRGAP_LATEST_SEQUENCE"
          echo "Airgap bundle images: $AIRGAP_BUNDLE_IMAGES"
          exit 0
        fi

        if [ "$AIRGAP_BUILD_STATUS" = "metadata" ]; then
          echo "Airgap has not been built yet. Triggering build..."
          replicated api post "v3/app/$APP_ID/channel/$CHANNEL_ID/release/$AIRGAP_LATEST_SEQUENCE/airgap/build"
        fi

        echo "Airgap build triggered. Polling every 10 seconds for up to 5 minutes..."
        for i in $(seq 1 30); do
          echo "Checking airgap build status... (attempt $i/30)"

          CHANNEL_RELEASES=$(replicated api get "v3/app/$APP_ID/channel/$CHANNEL_ID/releases")
          AIRGAP_BUILD_STATUS=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBuildStatus // "none"')
          AIRGAP_BUILD_ERROR=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBuildError // "none"')
          AIRGAP_BUNDLE_IMAGES=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].airgapBundleImages // "none"')
          AIRGAP_LATEST_SEQUENCE=$(echo "$CHANNEL_RELEASES" | jq -r '.releases[0].channelSequence')

          echo "Airgap build current status: $AIRGAP_BUILD_STATUS"

          if [ "$AIRGAP_BUILD_STATUS" = "built" ]; then
            echo "Airgap build completed successfully!"
            echo "Airgap bundle images: $AIRGAP_BUNDLE_IMAGES"
            exit 0
          fi
          sleep 10
        done

        echo "Timeout: Airgap build did not complete within 5 minutes."
        echo "Last build status: $AIRGAP_BUILD_STATUS"
        echo "Last build error: $AIRGAP_BUILD_ERROR"
        exit 1
