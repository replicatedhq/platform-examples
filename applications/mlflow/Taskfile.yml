version: '3'

# MLflow CI and Local Testing Taskfile
# This file centralizes all testing and CI tasks for the MLflow application

# Common variables
vars:
  # Directory structure
  CHART_DIR: ./charts
  RELEASE_DIR: ./release
  TESTS_DIR: ./tests
  
  # Testing configuration
  NAMESPACE: mlflow
  PORT: 5000
  
  # Chart configuration
  CHARTS: mlflow infra
  
  # Environment detection
  CI:
    sh: echo "${CI:-false}"
  
  # Docker configuration
  DEV_IMAGE: ttl.sh/mlflow-dev:dev
  CI_IMAGE_REGISTRY: ttl.sh
  CI_IMAGE_NAME: mlflow-dev
  CI_IMAGE_TAG:
    sh: git rev-parse --short HEAD 2>/dev/null || echo "dev"
  CI_IMAGE_TTL: 1h
  
  # Resource-related parameters (adjustable for local/CI environments)
  TIMEOUT: '{{if eq .CI "true"}}5m{{else}}3m{{end}}'
  WAIT_RETRIES: '{{if eq .CI "true"}}30{{else}}15{{end}}'
  RETRY_INTERVAL: '10'
  
  # Helm chart versions (dynamically determined)
  MLFLOW_VERSION:
    sh: helm show chart ./charts/mlflow | grep '^version:' | cut -d ' ' -f 2
  INFRA_VERSION:
    sh: helm show chart ./charts/infra | grep '^version:' | cut -d ' ' -f 2
    
  # Release configuration
  # APP_NAME can be overridden by setting REPLICATED_APP environment variable 
  APP_NAME: '{{.REPLICATED_APP | default "diamon-mlflow"}}'
  YAML_DIR: "./release"

# Default task shows help
tasks:
  default:
    desc: Show help information about available tasks
    cmds:
      - echo "MLflow CI and Testing Tasks"
      - echo "=========================="
      - task --list
    silent: true

  # Development environment tasks
  dev:build-image:
    desc: Build development Docker image
    silent: true
    cmds:
      - docker build -t {{.DEV_IMAGE}} -f Dockerfile .

  dev:start:
    desc: Start development container in background
    silent: true
    cmds:
      - echo "Starting development container..."
      - |
        # Check if container is running
        if docker ps | grep -q "mlflow-dev-container"; then
          echo "Container mlflow-dev-container is already running"
        # Check if container exists but is not running
        elif docker ps -a | grep -q "mlflow-dev-container"; then
          echo "Container mlflow-dev-container exists but is not running. Starting it..."
          docker start mlflow-dev-container
        # Create and start new container
        else
          # Try ports 5000-5004, use the first available one
          PORT_FOUND=false
          for PORT in {5000..5004}; do
            if ! lsof -i :$PORT > /dev/null; then
              PORT_ARG="-p $PORT:$PORT"
              echo "Using port $PORT for MLflow UI"
              PORT_FOUND=true
              break
            fi
          done

          # If all standard ports are in use, use a random port
          if [ "$PORT_FOUND" = "false" ]; then
            PORT_ARG="-P"
            echo "All standard ports are in use. Using a random port."
          fi

          # Add privileged mode if USE_KIND is true
          PRIVILEGED_ARG=""
          if [ "${USE_KIND:-false}" = "true" ]; then
            PRIVILEGED_ARG="--privileged"
          fi

          # Start container with host networking for kubectl port-forward compatibility
          CONTAINER_ID=$(docker run --name mlflow-dev-container --network host --privileged -d \
            -v $(pwd):/workspace \
            -v ~/.kube:/home/devuser/.kube \
            -v ~/.helm:/home/devuser/.helm \
            -v ~/.replicated:/home/devuser/.replicated \
            -e SHELL=/bin/bash \
            -e HOME=/home/devuser \
            -e USER=devuser \
            -e HOST_NETWORK=true \
            -w /workspace \
            {{.DEV_IMAGE}} sleep infinity)

          if [ $? -eq 0 ]; then
            echo "Development container started successfully with ID: $CONTAINER_ID"
            echo "Ports opened within the container will be directly accessible on your host machine."
          else
            echo "Failed to start development container"
            exit 1
          fi
        fi

  dev:shell:
    desc: Attach to development container shell
    deps: [dev:ensure-running]
    silent: true
    cmds:
      - echo "Connecting to mlflow-dev-container..."
      - docker exec -it mlflow-dev-container /bin/bash

  dev:ensure-running:
    desc: Ensure the development container is running
    internal: true
    silent: true
    cmds:
      - |
        if ! docker ps | grep -q "mlflow-dev-container"; then
          echo "Container mlflow-dev-container is not running. Starting it..."
          task dev:start
        else
          echo "Container mlflow-dev-container is already running."
        fi
    status:
      - docker ps | grep -q "mlflow-dev-container"

  dev:shell:kind:
    desc: Start container with Kind Kubernetes cluster and attach to shell
    deps: [dev:prepare-kind]
    silent: true
    cmds:
      - echo "Entering development container with Kind Kubernetes cluster..."
      - docker exec -it mlflow-dev-container /bin/bash

  dev:prepare-kind:
    desc: Prepare the development container for Kind usage
    internal: true
    silent: true
    cmds:
      - echo "Preparing development container with Kind Kubernetes cluster..."
      - |
        if docker ps | grep -q "mlflow-dev-container"; then
          echo "Container mlflow-dev-container is already running. Stopping it first..."
          docker stop mlflow-dev-container
          docker rm mlflow-dev-container
        elif docker ps -a | grep -q "mlflow-dev-container"; then
          echo "Container mlflow-dev-container exists but is not running. Removing it..."
          docker rm mlflow-dev-container
        fi
        
        # Start container with host networking and privileged mode for Kind
        docker run --name mlflow-dev-container --network host --privileged -d \
          -v $(pwd):/workspace \
          -v ~/.replicated:/home/devuser/.replicated \
          -e SHELL=/bin/bash \
          -e HOME=/home/devuser \
          -e USER=devuser \
          -e USE_KIND=true \
          -e HOST_NETWORK=true \
          -w /workspace \
          {{.DEV_IMAGE}} sleep infinity

  dev:stop:
    desc: Stop development container
    silent: true
    cmds:
      - |
        if docker ps | grep -q "mlflow-dev-container"; then
          echo "Stopping development container..."
          docker stop mlflow-dev-container
        else
          echo "Container mlflow-dev-container is not running"
        fi

  dev:restart:
    desc: Restart development container
    silent: true
    cmds:
      - task: dev:stop
      - task: dev:start

  # CI image tasks
  ci:build-image:
    desc: Build and tag a CI image with the current git SHA
    silent: true
    cmds:
      - echo "Building CI Docker image {{.CI_IMAGE_REGISTRY}}/{{.CI_IMAGE_NAME}}-{{.CI_IMAGE_TAG}}:{{.CI_IMAGE_TTL}}..."
      - docker build -t {{.CI_IMAGE_REGISTRY}}/{{.CI_IMAGE_NAME}}-{{.CI_IMAGE_TAG}}:{{.CI_IMAGE_TTL}} .
      - echo "✅ CI image built successfully"

  ci:push-image:
    desc: Push the CI image to a temporary registry (ttl.sh)
    deps: [ci:build-image]
    silent: true
    cmds:
      - echo "Pushing CI Docker image to {{.CI_IMAGE_REGISTRY}}..."
      - docker push {{.CI_IMAGE_REGISTRY}}/{{.CI_IMAGE_NAME}}-{{.CI_IMAGE_TAG}}:{{.CI_IMAGE_TTL}}
      - echo "✅ CI image pushed successfully to {{.CI_IMAGE_REGISTRY}}/{{.CI_IMAGE_NAME}}-{{.CI_IMAGE_TAG}}:{{.CI_IMAGE_TTL}}"

  # Version checking task
  versions:verify:
    desc: Check if versions match between Chart.yaml files and HelmChart manifests
    silent: true
    cmds:
      - echo "Checking chart versions consistency..."
      - |
        # For each available chart, check that versions match
        all_match=true
        
        for chart in {{.CHARTS}}; do
          echo "Checking $chart chart..."
          
          # Get the Chart.yaml path
          chart_yaml="{{.CHART_DIR}}/$chart/Chart.yaml"
          
          # Get the HelmChart resource path
          helmchart="{{.RELEASE_DIR}}/$chart-chart.yaml"
          
          # Check if both files exist
          if [ ! -f "$chart_yaml" ]; then
            echo "❌ Chart.yaml not found: $chart_yaml"
            all_match=false
            continue
          fi
          
          if [ ! -f "$helmchart" ]; then
            echo "❌ HelmChart resource not found: $helmchart"
            all_match=false
            continue
          fi
          
          # Get versions from both files
          chart_yaml_version=$(yq e '.version' "$chart_yaml")
          helmchart_version=$(yq e '.spec.chart.chartVersion' "$helmchart")
          
          # Check if versions could be extracted
          if [ -z "$chart_yaml_version" ]; then
            echo "❌ Could not extract version from Chart.yaml: $chart_yaml"
            all_match=false
            continue
          fi
          
          if [ -z "$helmchart_version" ]; then
            echo "❌ Could not extract chartVersion from HelmChart: $helmchart"
            all_match=false
            continue
          fi
          
          # Compare versions
          if [ "$chart_yaml_version" = "$helmchart_version" ]; then
            echo "✅ Versions match for $chart: $chart_yaml_version"
          else
            echo "❌ Version mismatch for $chart:"
            echo "   Chart.yaml version: $chart_yaml_version"
            echo "   HelmChart version: $helmchart_version"
            all_match=false
          fi
        done
        
        # Exit with error if any versions don't match
        if [ "$all_match" = true ]; then
          echo "✅ All chart versions match between Chart.yaml and HelmChart resources."
        else
          echo "❌ Version mismatches found! Please run 'task release:update-versions' to synchronize them."
          exit 1
        fi
    status:
      - |
        # For each available chart, check that versions match
        for chart in {{.CHARTS}}; do
          # Get the Chart.yaml path
          chart_yaml="{{.CHART_DIR}}/$chart/Chart.yaml"
          
          # Get the HelmChart resource path
          helmchart="{{.RELEASE_DIR}}/$chart-chart.yaml"
          
          # Check if both files exist
          if [ ! -f "$chart_yaml" ] || [ ! -f "$helmchart" ]; then
            exit 1
          fi
          
          # Get versions from both files
          chart_yaml_version=$(yq e '.version' "$chart_yaml")
          helmchart_version=$(yq e '.spec.chart.chartVersion' "$helmchart")
          
          # Check if versions could be extracted
          if [ -z "$chart_yaml_version" ] || [ -z "$helmchart_version" ]; then
            exit 1
          fi
          
          # Compare versions
          if [ "$chart_yaml_version" != "$helmchart_version" ]; then
            exit 1
          fi
        done
        # All versions match
        exit 0

  # Dependency update - renamed
  helm:update-deps:
    desc: Update Helm chart dependencies
    silent: true
    cmds:
      - echo "Updating Helm chart dependencies..."
      - for: { var: CHARTS }
        cmd: |
          echo "Updating dependencies for {{.ITEM}} chart..."
          helm dependency update {{.CHART_DIR}}/{{.ITEM}}
      - echo "Helm chart dependencies updated successfully."
    status:
      - |
        # Check if all charts have up-to-date dependencies
        for chart in {{.CHARTS}}; do
          # Check if Chart.lock exists and is newer than Chart.yaml
          if [[ ! -f "{{.CHART_DIR}}/$chart/Chart.lock" || "{{.CHART_DIR}}/$chart/Chart.yaml" -nt "{{.CHART_DIR}}/$chart/Chart.lock" ]]; then
            # Chart.lock doesn't exist or is older than Chart.yaml, need to update
            exit 1
          fi
        done
        # All charts have up-to-date dependencies
        exit 0

  # Lint charts
  helm:lint:
    desc: Lint Helm charts
    silent: true
    cmds:
      - echo "Linting Helm charts..."
      - |
        # For each chart, run helm lint with path
        for chart in {{.CHARTS}}; do
          echo "Linting $chart chart..."
          helm lint {{.CHART_DIR}}/$chart
        done
      - echo "✅ Helm lint check complete."

  # Template rendering
  helm:template:
    desc: Template Helm charts with standard configuration and output to a directory
    silent: true
    cmds:
      - echo "Rendering Helm chart templates..."
      - |
        # Create directory for rendered templates
        mkdir -p {{.CHART_DIR}}/.rendered-templates
        
        # For each chart, render templates
        for chart in {{.CHARTS}}; do
          echo "Rendering templates for $chart chart..."
          
          # Create output directory
          OUTPUT_DIR="{{.CHART_DIR}}/.rendered-templates/$chart"
          mkdir -p "$OUTPUT_DIR"
          
          # Render templates
          helm template "$chart" {{.CHART_DIR}}/$chart \
            --namespace {{.NAMESPACE}} \
            --set replicated.enabled=false \
            --output-dir "$OUTPUT_DIR"
        done
      - echo "✅ Templates rendered successfully to {{.CHART_DIR}}/.rendered-templates"

  # Update chart versions
  release:update-versions:
    desc: Update chart version references in release manifests
    silent: true
    cmds:
      - echo "Updating chart version references in release manifests..."
      - |
        # Get chart versions
        MLFLOW_VERSION="{{.MLFLOW_VERSION}}"
        INFRA_VERSION="{{.INFRA_VERSION}}"
        
        echo "Working with chart versions:"
        echo "MLflow chart version: $MLFLOW_VERSION"
        echo "Infra chart version: $INFRA_VERSION"
        
        # Update MLflow HelmChart manifest
        MLFLOW_HELMCHART="{{.RELEASE_DIR}}/mlflow-chart.yaml"
        if [ -f "$MLFLOW_HELMCHART" ]; then
          echo "Updating version in $MLFLOW_HELMCHART to $MLFLOW_VERSION..."
          
          if command -v yq &> /dev/null; then
            # Use yq if available
            yq eval ".spec.chart.chartVersion = \"$MLFLOW_VERSION\"" -i "$MLFLOW_HELMCHART"
          else
            # Fallback to sed
            sed -i.bak "s/chartVersion:.*/chartVersion: \"$MLFLOW_VERSION\"/" "$MLFLOW_HELMCHART" && rm "${MLFLOW_HELMCHART}.bak"
          fi
        else
          echo "⚠️ MLflow HelmChart not found at $MLFLOW_HELMCHART"
        fi
        
        # Update Infra HelmChart manifest
        INFRA_HELMCHART="{{.RELEASE_DIR}}/infra-chart.yaml"
        if [ -f "$INFRA_HELMCHART" ]; then
          echo "Updating version in $INFRA_HELMCHART to $INFRA_VERSION..."
          
          if command -v yq &> /dev/null; then
            # Use yq if available
            yq eval ".spec.chart.chartVersion = \"$INFRA_VERSION\"" -i "$INFRA_HELMCHART"
          else
            # Fallback to sed
            sed -i.bak "s/chartVersion:.*/chartVersion: \"$INFRA_VERSION\"/" "$INFRA_HELMCHART" && rm "${INFRA_HELMCHART}.bak"
          fi
        else
          echo "⚠️ Infra HelmChart not found at $INFRA_HELMCHART"
        fi
        
        echo "✅ Release manifest versions updated successfully."
      - cmd: task versions:verify || echo "⚠️ Version check failed after update. Please verify manually."
        ignore_error: true

  # Package charts
  helm:package:
    desc: Package Helm charts for distribution
    silent: true
    deps: [helm:update-deps, release:update-versions]
    cmds:
      - echo "Packaging Helm charts for distribution..."
      - |
        # For each chart, package it
        for chart in {{.CHARTS}}; do
          echo "Packaging $chart chart..."
          cd {{.CHART_DIR}}/$chart && helm package .
          
          # Move resulting .tgz file to release directory
          mv *.tgz ../../release/
        done
      - echo "✅ Charts packaged successfully. Chart archives are in the release directory."

  # Create a release in the Replicated vendor portal
  release:create:
    desc: Create a release in Replicated
    deps: [versions:verify, helm:package]
    silent: true
    vars:
      VERSION: '{{.VERSION | default .MLFLOW_VERSION}}'
      REPLICATED_CHANNEL: '{{.REPLICATED_CHANNEL | default ""}}'
    cmds:
      - echo "Creating release version {{.VERSION}} for app {{.APP_NAME}}..."
      - |
        if [ -z "{{.REPLICATED_CHANNEL}}" ]; then
          echo "❌ Error: No channel specified. Please provide a channel with REPLICATED_CHANNEL=your-channel-name"
          exit 1
        fi
        
        echo "Creating release for app {{.APP_NAME}} in channel {{.REPLICATED_CHANNEL}} with version {{.VERSION}}"
        replicated release create \
          --yaml-dir {{.YAML_DIR}} \
          --promote {{.REPLICATED_CHANNEL}} \
          --ensure-channel \
          --version {{.VERSION}} \
          --release-notes "{{.RELEASE_NOTES}}" \
          --app {{.APP_NAME}}
        
        if [ $? -eq 0 ]; then
          echo "✅ Release {{.VERSION}} created successfully and promoted to channel {{.REPLICATED_CHANNEL}}"
        else
          echo "❌ Failed to create release"
          exit 1
        fi

  # Cleanup tasks
  clean:charts:
    desc: Clean packaged charts from release directory
    silent: true
    cmds:
      - rm -f {{.RELEASE_DIR}}/*.tgz
      - echo "Chart packages cleaned from {{.RELEASE_DIR}}"

  clean:templates:
    desc: Clean rendered templates directory
    silent: true
    cmds:
      - rm -rf {{.CHART_DIR}}/.rendered-templates
      - echo "Rendered templates cleaned from {{.CHART_DIR}}/.rendered-templates"

  # Main clean task
  clean:all:
    desc: Clean all generated files
    deps: [clean:charts, clean:templates]
    silent: true
    cmds:
      - echo "All generated files cleaned successfully"

  # Helm test task
  helm:test-install:
    desc: Run Helm installation test from Replicated registry
    deps: [registry:login, namespaces:create]
    silent: true
    cmds:
      - echo "Running Helm installation test with custom values..."
      - |
        # Determine OCI URL - prefer direct OCI_URL if provided, otherwise construct from app/channel
        if [ -n "$OCI_URL" ]; then
          echo "Using provided OCI URL: $OCI_URL"
        else
          echo "No direct OCI_URL provided. Constructing from REPLICATED_APP and REPLICATED_CHANNEL"
          echo "Note: This requires REPLICATED_APP and REPLICATED_CHANNEL env vars."
          if [ -z "$REPLICATED_APP" ] || [ -z "$REPLICATED_CHANNEL" ]; then
            echo "ERROR: REPLICATED_APP and REPLICATED_CHANNEL must be set"
            exit 1
          fi
          OCI_URL="oci://registry.replicated.com/$REPLICATED_APP/$REPLICATED_CHANNEL"
          echo "Constructed OCI URL: $OCI_URL"
        fi
        
        # Validate OCI_URL is set and not empty
        if [ -z "$OCI_URL" ]; then
          echo "ERROR: OCI_URL is empty. Check that REPLICATED_APP and REPLICATED_CHANNEL are correctly set."
          echo "REPLICATED_APP=$REPLICATED_APP"
          echo "REPLICATED_CHANNEL=$REPLICATED_CHANNEL"
          exit 1
        fi
        
        # Prepare values arguments if provided
        MLFLOW_VALUES_ARGS=""
        if [ -n "$MLFLOW_VALUES" ]; then
          echo "Using MLflow values file: $MLFLOW_VALUES"
          # Check if values file exists
          if [ ! -f "$MLFLOW_VALUES" ]; then
            echo "ERROR: Values file '$MLFLOW_VALUES' does not exist"
            exit 1
          fi
          MLFLOW_VALUES_ARGS="--values $MLFLOW_VALUES"
          echo "Values args: $MLFLOW_VALUES_ARGS"
        else
          echo "No custom values file provided. Using default values."
        fi
        
        # Clean up any existing port forwarding first
        task helm:port:cleanup || true
        
        # Install infra chart from Replicated registry
        echo "Installing infra chart from Replicated registry..."
        echo "Chart path: $OCI_URL/infra"
        helm upgrade --install infra $OCI_URL/infra \
          --namespace {{.NAMESPACE}} \
          --wait --timeout {{.TIMEOUT}} --debug || { 
          echo "ERROR: Failed to install infra chart from $OCI_URL/infra"
          echo "Please check that registry login was successful and the chart exists in the registry."
          exit 1
        }
        
        # Install MLflow chart from Replicated registry with custom values
        echo "Installing mlflow chart from Replicated registry with custom values..."
        echo "Chart path: $OCI_URL/mlflow"
        echo "Using values args: $MLFLOW_VALUES_ARGS"
        helm upgrade --install mlflow $OCI_URL/mlflow \
          --namespace {{.NAMESPACE}} \
          $MLFLOW_VALUES_ARGS \
          --wait --timeout {{.TIMEOUT}} --debug || { 
          echo "ERROR: Failed to install mlflow chart from $OCI_URL/mlflow"
          echo "Please check that registry login was successful and the chart exists in the registry."
          exit 1
        }
        
        echo "Helm installation with custom values completed successfully."
      - task: helm:port:forward

  # Replicated KOTS test task
  kots:test-install:
    desc: Run Replicated KOTS installation test
    deps: [namespaces:create]
    silent: true
    cmds:
      - echo "Running Replicated KOTS installation test..."
      - |
        if [ -z "$REPLICATED_LICENSE_ID" ]; then
          echo "ERROR: REPLICATED_LICENSE_ID environment variable must be set"
          exit 1
        fi

        if [ -z "$REPLICATED_APP" ]; then
          echo "ERROR: REPLICATED_APP environment variable must be set"
          exit 1
        fi

        if [ -z "$REPLICATED_CHANNEL" ]; then
          echo "ERROR: REPLICATED_CHANNEL environment variable must be set"
          exit 1
        fi
        
        # Clean up any existing port forwarding first
        task helm:port:cleanup || true
        
        # Create directory for license file if it doesn't exist
        mkdir -p /tmp/replicated
        LICENSE_FILE="/tmp/replicated/license.yaml"
        
        # Validate license file exists and has content
        if [ ! -f "$LICENSE_FILE" ] || [ ! -s "$LICENSE_FILE" ]; then
          echo "ERROR: License file does not exist or is empty at $LICENSE_FILE"
          echo "Please download the license file using the license:download task first"
          exit 1
        fi
        
        # Verify license file is valid YAML
        if command -v yq >/dev/null 2>&1; then
          echo "Validating license file is proper YAML before installation..."
          if ! yq eval . "$LICENSE_FILE" > /dev/null 2>&1; then
            echo "ERROR: License file is not valid YAML"
            echo "License file content:"
            cat "$LICENSE_FILE" | head -n 10
            exit 1
          else
            echo "✅ License file is valid YAML"
          fi
        else
          echo "WARNING: yq not found, skipping YAML validation"
        fi
        
        echo "Installing latest KOTS version..."
        curl https://kots.io/install | bash
        
        echo "License file at $LICENSE_FILE (first 5 lines):"
        head -n 5 "$LICENSE_FILE"
        
        echo "Installing application from Replicated..."
        echo "App: $REPLICATED_APP"
        echo "Channel: $REPLICATED_CHANNEL"
        echo "Using license file: $LICENSE_FILE"
        
        # Run KOTS install with detailed output
        set -x
        kubectl kots install $REPLICATED_APP/$REPLICATED_CHANNEL \
          --shared-password=replicatedmlflow \
          --license-file="$LICENSE_FILE" \
          --namespace=default \
          --wait-duration=10m \
          --skip-preflights
        set +x
        
        # Check if installation succeeded
        if [ $? -ne 0 ]; then
          echo "❌ KOTS installation failed"
          echo "Checking app status:"
          kubectl get app -n default
          echo "Checking pods:"
          kubectl get pods -n default
          echo "Checking pod logs:"
          kubectl logs -n default -l app=kotsadm --tail=50
          exit 1
        fi
        
        echo "✅ Replicated installation completed. Setting up port forwarding for testing..."
      - task: helm:port:forward

  # Port forwarding task - now broken into smaller sub-tasks
  helm:port:setup:
    desc: Set up port forwarding to MLflow service
    internal: true
    silent: true
    cmds:
      - echo "Setting up port forwarding to MLflow service..."
      - |
        SERVICE_NAME=$(kubectl get svc -n {{.NAMESPACE}} -l app.kubernetes.io/name=mlflow -o name | head -n 1)
        if [ -z "$SERVICE_NAME" ]; then
          echo "ERROR: Could not find MLflow service with label app.kubernetes.io/name=mlflow"
          exit 1
        fi
        
        echo "Setting up port forwarding to $SERVICE_NAME..."
        # Set up port forwarding in the background with logs
        echo "Setting up port forwarding using nohup..."
        PORT_FORWARD_LOG="/tmp/port-forward-mlflow-$$.log"
        nohup kubectl port-forward -n {{.NAMESPACE}} $SERVICE_NAME {{.PORT}}:5000 > $PORT_FORWARD_LOG 2>&1 &
        PORT_FORWARD_PID=$!
        
        # Give port-forward a moment to start
        sleep 2
        
        # Verify the PID was captured properly
        if [ -z "$PORT_FORWARD_PID" ] || [ "$PORT_FORWARD_PID" = "0" ]; then
          echo "ERROR: Failed to capture port-forward process PID"
          echo "Attempting alternate port forwarding method..."
          
          # Alternative approach - use a fixed port file to track the PID
          PID_FILE="/tmp/mlflow-portforward.pid"
          rm -f $PID_FILE
          
          # Use a background task with PID file
          (
            kubectl port-forward -n {{.NAMESPACE}} $SERVICE_NAME {{.PORT}}:5000 > $PORT_FORWARD_LOG 2>&1 &
            echo $! > $PID_FILE
            wait
          ) &
          
          # Wait a moment and check if the PID file was created
          sleep 3
          if [ -f $PID_FILE ]; then
            PORT_FORWARD_PID=$(cat $PID_FILE)
            echo "Port forwarding set up with alternate method, PID: $PORT_FORWARD_PID"
          else
            echo "ERROR: Both port forwarding methods failed. Continuing anyway..."
            # Continue anyway and rely on curl checks to verify connectivity
            PORT_FORWARD_PID=""
          fi
        else
          echo "Port forwarding set up with PID: $PORT_FORWARD_PID"
        fi
        
        # Save the PID to a file for other tasks to use
        echo $PORT_FORWARD_PID > /tmp/mlflow-port-forward-main.pid
        echo "✅ Port forwarding initialized."

  helm:port:check:
    desc: Check if port forwarding is working and restart if needed
    internal: true
    silent: true
    cmds:
      - echo "Checking port forwarding status..."
      - |
        # Read PID from file if it exists
        PORT_FORWARD_LOG="/tmp/port-forward-mlflow-$$.log"
        if [ -f "/tmp/mlflow-port-forward-main.pid" ]; then
          PORT_FORWARD_PID=$(cat /tmp/mlflow-port-forward-main.pid)
        else
          echo "WARNING: No PID file found for port forwarding process"
          PORT_FORWARD_PID=""
        fi
        
        # Give port-forward more time to establish
        echo "Waiting for port-forward to establish..."
        sleep 5
        
        # Only check process if we have a PID
        if [ -n "$PORT_FORWARD_PID" ]; then
          # Check if port-forward process is still running
          if ! ps -p $PORT_FORWARD_PID > /dev/null 2>&1; then
            echo "WARNING: Port forwarding process with PID $PORT_FORWARD_PID is not running"
            echo "Port forwarding log:"
            cat $PORT_FORWARD_LOG || echo "No log file found"
            echo "Will try to restart port forwarding..."
            
            # Get service name again
            SERVICE_NAME=$(kubectl get svc -n {{.NAMESPACE}} -l app.kubernetes.io/name=mlflow -o name | head -n 1)
            if [ -z "$SERVICE_NAME" ]; then
              echo "ERROR: Could not find MLflow service to restart port forwarding"
              echo "Will try to connect anyway..."
            else
              # Restart port forwarding
              echo "Attempting to restart port forwarding..."
              nohup kubectl port-forward -n {{.NAMESPACE}} $SERVICE_NAME {{.PORT}}:5000 > $PORT_FORWARD_LOG 2>&1 &
              PORT_FORWARD_PID=$!
              echo $PORT_FORWARD_PID > /tmp/mlflow-port-forward-main.pid
              sleep 3
              
              if [ -z "$PORT_FORWARD_PID" ] || [ "$PORT_FORWARD_PID" = "0" ]; then
                echo "WARNING: Failed to capture restarted port-forward process PID"
                echo "Will continue without checking process status"
              else
                echo "Restarted port forwarding with PID: $PORT_FORWARD_PID"
              fi
              
              sleep 5 # Give the new port-forward time to establish
            fi
          else
            echo "✅ Port forwarding process is running with PID: $PORT_FORWARD_PID"
          fi
        fi
        echo "✅ Port forwarding check completed."

  helm:port:test:
    desc: Test connectivity to MLflow service
    internal: true
    silent: true
    cmds:
      - echo "Testing connectivity to MLflow on localhost:{{.PORT}}..."
      - |
        # Basic connectivity check
        MAX_CONN_RETRIES=5
        CONN_RETRY_COUNT=0
        CONN_SUCCESS=false
        
        while [ $CONN_RETRY_COUNT -lt $MAX_CONN_RETRIES ]; do
          CONN_RETRY_COUNT=$((CONN_RETRY_COUNT+1))
          echo "Connection attempt $CONN_RETRY_COUNT/$MAX_CONN_RETRIES..."
          
          # Try curling the MLflow endpoint
          if curl -s -o /dev/null -w "%{http_code}" http://localhost:{{.PORT}}/ > /dev/null 2>&1; then
            echo "✅ Successfully connected to MLflow service!"
            CONN_SUCCESS=true
            break
          else
            echo "Connection attempt $CONN_RETRY_COUNT failed, retrying in 5 seconds..."
            
            # Check if port-forward is still running
            if [ -f "/tmp/mlflow-port-forward-main.pid" ]; then
              PORT_FORWARD_PID=$(cat /tmp/mlflow-port-forward-main.pid)
              if [ -n "$PORT_FORWARD_PID" ] && ! ps -p $PORT_FORWARD_PID > /dev/null 2>&1; then
                echo "WARNING: Port forwarding process with PID $PORT_FORWARD_PID is not running"
                echo "Port forwarding log:"
                cat /tmp/port-forward-mlflow-$$.log || echo "No log file found"
                echo "Will try to connect anyway..."
              fi
            fi
            
            sleep 5
          fi
        done
        
        if [ "$CONN_SUCCESS" != "true" ]; then
          echo "WARNING: Could not connect to MLflow service after $MAX_CONN_RETRIES attempts."
          echo "This may indicate issues with the service or port forwarding."
          
          if [ -f "/tmp/port-forward-mlflow-$$.log" ]; then
            echo "Port forwarding log:"
            cat /tmp/port-forward-mlflow-$$.log
          fi
          
          echo "Pod logs:"
          kubectl logs -n {{.NAMESPACE}} -l app.kubernetes.io/name=mlflow --tail=20 || true
          echo "Continuing anyway, but tests may fail."
        fi
        echo "✅ Connectivity test completed."

  helm:port:forward:
    desc: Setup port forwarding to MLflow service for testing
    internal: true
    deps: [helm:service:find, helm:pods:wait]
    silent: true
    cmds:
      - echo "Setting up port forwarding to MLflow service..."
      - task: helm:port:setup
      - task: helm:port:check
      - task: helm:port:test
      - echo "✅ Port forwarding setup completed successfully."
    status:
      - |
        # Check if port forwarding is already active
        if [ -f "/tmp/mlflow-port-forward-main.pid" ]; then
          PID=$(cat /tmp/mlflow-port-forward-main.pid)
          if ps -p $PID > /dev/null && lsof -i:{{.PORT}} | grep -q LISTEN; then
            # Port forwarding is active
            exit 0
          fi
        fi
        # Port forwarding is not active
        exit 1

  helm:port:cleanup:
    desc: Clean up port forwarding processes
    silent: true
    cmds:
      - echo "Cleaning up port forwarding processes..."
      - |
        # Check if PID file exists
        if [ -f "/tmp/mlflow-port-forward-main.pid" ]; then
          PID=$(cat /tmp/mlflow-port-forward-main.pid)
          
          # Check if process is running and kill it
          if [ -n "$PID" ] && ps -p $PID > /dev/null 2>&1; then
            echo "Terminating port forwarding process with PID: $PID"
            kill $PID
            if [ $? -eq 0 ]; then
              echo "✅ Port forwarding process terminated successfully"
            else
              echo "❌ Failed to terminate port forwarding process"
            fi
          else
            echo "No running port forwarding process found with PID: $PID"
          fi
          
          # Remove PID file
          rm -f /tmp/mlflow-port-forward-main.pid
          echo "Removed PID file"
        else
          echo "No port forwarding PID file found"
        fi
        
        # Clean up any log files
        rm -f /tmp/port-forward-mlflow-*.log
        
        # Check for any lingering port-forward processes
        echo "Checking for lingering port-forward processes on port {{.PORT}}..."
        RUNNING_FORWARDS=$(ps aux | grep "port-forward.*:{{.PORT}}" | grep -v grep || true)
        if [ -n "$RUNNING_FORWARDS" ]; then
          echo "Found running port-forward processes:"
          echo "$RUNNING_FORWARDS"
          echo "Attempting to kill these processes..."
          
          # Extract PIDs and kill
          ps aux | grep "port-forward.*:{{.PORT}}" | grep -v grep | awk '{print $2}' | xargs -r kill
          echo "✅ Lingering port-forward processes terminated"
        else
          echo "No lingering port-forward processes found"
        fi
        
        echo "✅ Port forwarding cleanup completed"
    status:
      - |
        # Check if there's no port forwarding to clean up
        if [ ! -f "/tmp/mlflow-port-forward-main.pid" ] && ! ps aux | grep "port-forward.*:{{.PORT}}" | grep -v grep | grep -q .; then
          # No port forwarding processes found
          exit 0
        fi
        # Port forwarding processes exist that need to be cleaned up
        exit 1

  # Local installation task
  helm:install-local:
    desc: Install MLflow with local Helm charts for development (with Replicated SDK disabled)
    deps: [namespaces:create]
    silent: true
    cmds:
      - echo "Installing MLflow with local Helm charts (Replicated SDK disabled)..."
      - |
        # Prepare values arguments if provided
        MLFLOW_VALUES_ARGS=""
        if [ -n "$MLFLOW_VALUES" ]; then
          echo "Using MLflow values file: $MLFLOW_VALUES"
          # Check if values file exists
          if [ ! -f "$MLFLOW_VALUES" ]; then
            echo "ERROR: Values file '$MLFLOW_VALUES' does not exist"
            exit 1
          fi
          MLFLOW_VALUES_ARGS="--values $MLFLOW_VALUES"
          echo "Values args: $MLFLOW_VALUES_ARGS"
        else
          echo "No custom values file provided. Using default values."
        fi
        
        # Clean up any existing port forwarding first
        task helm:port:cleanup || true
        
        # Install infra chart from local directory
        echo "Installing infra chart from local directory..."
        helm upgrade --install infra {{.CHART_DIR}}/infra \
          --namespace {{.NAMESPACE}} \
          --wait --timeout {{.TIMEOUT}} --debug || { 
          echo "ERROR: Failed to install infra chart from {{.CHART_DIR}}/infra"
          exit 1
        }
        
        # Install MLflow chart from local directory with custom values
        # Note: We explicitly disable the Replicated SDK for local development to avoid
        # dependencies on the Replicated platform during development
        echo "Installing mlflow chart from local directory (Replicated SDK disabled)..."
        echo "Using values args: $MLFLOW_VALUES_ARGS"
        helm upgrade --install mlflow {{.CHART_DIR}}/mlflow \
          --namespace {{.NAMESPACE}} \
          --set replicated.enabled=false \
          $MLFLOW_VALUES_ARGS \
          --wait --timeout {{.TIMEOUT}} --debug || { 
          echo "ERROR: Failed to install mlflow chart from {{.CHART_DIR}}/mlflow"
          exit 1
        }
        
        echo "Local Helm installation completed successfully."
      - task: helm:port:forward

  # App test task
  test:app:
    desc: Run application tests against the running MLflow service
    silent: true
    cmds:
      - echo "Running MLflow application tests against localhost:{{.PORT}}..."
      - |
        # Detect if we're running in a CI environment
        if [ "{{.CI}}" = "true" ]; then
          echo "📦 Running in CI environment - using direct package installation..."
          
          # In CI, we just install packages directly without using a virtual environment
          echo "Installing Python dependencies directly..."
          python -m pip install --upgrade pip wheel setuptools
          
          # Install required packages directly
          echo "Installing MLflow and test dependencies..."
          python -m pip install "mlflow>=2.8.0,<3.0.0" "numpy>=1.24.0" "pandas>=2.0.0" "scikit-learn>=1.2.0" pytest requests
          
          # Run the tests directly
          echo "🧪 Running MLflow application tests..."
          if python {{.TESTS_DIR}}/mlflow_test.py localhost:{{.PORT}} --protocol http --connection-timeout 180 --debug; then
            echo "✅ All tests passed successfully!"
          else
            TEST_EXIT_CODE=$?
            echo "❌ Tests failed with exit code: $TEST_EXIT_CODE"
            exit $TEST_EXIT_CODE
          fi
        else
          # For local development, use a persistent virtual environment for better isolation and speed
          echo "🔧 Setting up Python test environment..."
          TEST_ENV_DIR="{{.TESTS_DIR}}/.venv"
          
          # Create virtual environment if it doesn't exist
          if [ ! -d "$TEST_ENV_DIR" ]; then
            echo "  Creating new Python environment (first-time setup)..."
            python3 -m venv "$TEST_ENV_DIR" || {
              echo "❌ Failed to create Python virtual environment."
              echo "   Please ensure python3 and python3-venv are installed."
              echo "   On Ubuntu/Debian: sudo apt-get install python3-venv"
              echo "   On macOS: brew install python3"
              exit 1
            }
            FRESH_ENV=true
          else
            echo "  Using existing Python environment from $TEST_ENV_DIR"
            FRESH_ENV=false
          fi
          
          # Determine the correct activation script based on shell
          if [ -f "$TEST_ENV_DIR/bin/activate" ]; then
            ACTIVATE_SCRIPT="$TEST_ENV_DIR/bin/activate"
          elif [ -f "$TEST_ENV_DIR/Scripts/activate" ]; then
            ACTIVATE_SCRIPT="$TEST_ENV_DIR/Scripts/activate"
          else
            echo "❌ Unable to find activation script for virtual environment"
            exit 1
          fi
          
          # Activate the virtual environment
          echo "  Activating test environment..."
          source "$ACTIVATE_SCRIPT" || {
            echo "❌ Failed to activate virtual environment."
            echo "   Trying alternative approach..."
            
            # Alternative approach using python -m venv approach
            echo "   Using python directly from the venv bin directory..."
            VENV_PYTHON="$TEST_ENV_DIR/bin/python"
            if [ ! -f "$VENV_PYTHON" ]; then
              if [ -f "$TEST_ENV_DIR/Scripts/python.exe" ]; then
                VENV_PYTHON="$TEST_ENV_DIR/Scripts/python.exe"
              else
                echo "❌ Cannot find python in the virtual environment."
                echo "   Falling back to system Python..."
                VENV_PYTHON="python"
              fi
            fi
            
            # Install using the venv python directly
            echo "   Installing dependencies using $VENV_PYTHON..."
            "$VENV_PYTHON" -m pip install --upgrade pip wheel setuptools
            "$VENV_PYTHON" -m pip install "mlflow>=2.8.0,<3.0.0" "numpy>=1.24.0" "pandas>=2.0.0" "scikit-learn>=1.2.0" pytest requests
            
            # Run the tests using venv python
            echo "🧪 Running MLflow application tests..."
            if "$VENV_PYTHON" {{.TESTS_DIR}}/mlflow_test.py localhost:{{.PORT}} --protocol http --connection-timeout 180 --debug; then
              echo "✅ All tests passed successfully!"
            else
              TEST_EXIT_CODE=$?
              echo "❌ Tests failed with exit code: $TEST_EXIT_CODE"
              exit $TEST_EXIT_CODE
            fi
            
            echo "💡 Environment is persistent for faster future runs."
            echo "   To force dependency updates: FORCE_DEPS_UPDATE=yes task test:app"
            echo "   To clean up environment: task venv:clean"
            
            # Exit early since we've already run the tests
            exit 0
          }
          
          # Only install/upgrade packages if it's a fresh environment or forced
          if [ "$FRESH_ENV" = true ] || [ "${FORCE_DEPS_UPDATE:-no}" = "yes" ]; then
            # Install dependencies with detailed progress
            echo "🔄 Installing required dependencies..."
            echo "  Upgrading package tools..."
            python -m pip install --upgrade pip wheel setuptools &> "$TEST_ENV_DIR/pip-upgrade.log" || {
              echo "❌ Failed to upgrade pip/wheel/setuptools."
              echo "   See error log at: $TEST_ENV_DIR/pip-upgrade.log"
              cat "$TEST_ENV_DIR/pip-upgrade.log"
              exit 1
            }
            
            echo "  Installing MLflow and test dependencies (this may take a minute)..."
            # Install all dependencies with a single command to resolve dependency conflicts properly
            python -m pip install "mlflow>=2.8.0,<3.0.0" "numpy>=1.24.0" "pandas>=2.0.0" "scikit-learn>=1.2.0" pytest requests &> "$TEST_ENV_DIR/pip-install.log" || {
              echo "❌ Failed to install dependencies."
              echo "   See error log at: $TEST_ENV_DIR/pip-install.log"
              echo "   Common issues:"
              echo "   - Python version compatibility"
              echo "   - Network connectivity problems"
              echo "   - System package dependencies missing"
              echo ""
              echo "Error details:"
              tail -n 20 "$TEST_ENV_DIR/pip-install.log"
              exit 1
            }
            
            # Show the installed versions
            echo "✅ Successfully installed dependencies:"
            python -m pip list | grep -E "mlflow|numpy|pandas|scikit-learn|pytest|requests"
          else
            echo "🔍 Using existing dependencies (use FORCE_DEPS_UPDATE=yes to update)"
          fi
          
          # Run the tests with proper error handling
          echo "🧪 Running MLflow application tests..."
          if python {{.TESTS_DIR}}/mlflow_test.py localhost:{{.PORT}} --protocol http --connection-timeout 180 --debug; then
            echo "✅ All tests passed successfully!"
          else
            TEST_EXIT_CODE=$?
            echo "❌ Tests failed with exit code: $TEST_EXIT_CODE"
            echo "   Check the test output above for details."
            exit $TEST_EXIT_CODE
          fi
          
          # Note about cleaning up
          echo "💡 Environment is persistent for faster future runs."
          echo "   To force dependency updates: FORCE_DEPS_UPDATE=yes task test:app"
          echo "   To clean up environment: task venv:clean"
        fi

  # All tests task
  test:all:
    desc: Run all tests
    deps: [helm:test-install, test:app]
    silent: true
    cmds:
      - echo "All tests completed successfully"
      - task: helm:port:cleanup

  # Alternative test task with venv
  test:app:venv:
    desc: Run application tests using a virtual environment for better isolation
    silent: true
    cmds:
      - echo "Running application tests in a virtual environment..."
      - |
        # Set up test env directory
        TEST_ENV_DIR="{{.TESTS_DIR}}/.venv"
        
        # Clean up any existing venv if requested
        if [ "${CLEAN_VENV:-no}" = "yes" ]; then
          echo "Cleaning up existing virtual environment..."
          rm -rf "$TEST_ENV_DIR"
        fi
        
        # Create virtual environment if it doesn't exist
        if [ ! -d "$TEST_ENV_DIR" ]; then
          echo "Setting up new virtual environment..."
          python -m venv "$TEST_ENV_DIR"
        fi
        
        # Determine the correct activation script based on shell
        if [ -f "$TEST_ENV_DIR/bin/activate" ]; then
          ACTIVATE_SCRIPT="$TEST_ENV_DIR/bin/activate"
        elif [ -f "$TEST_ENV_DIR/Scripts/activate" ]; then
          ACTIVATE_SCRIPT="$TEST_ENV_DIR/Scripts/activate"
        else
          echo "❌ Unable to find activation script for virtual environment"
          exit 1
        fi
        
        # Create a temporary script to run in the activated environment
        TMP_SCRIPT=$(mktemp)
        cat > "$TMP_SCRIPT" << 'EOF'
        set -e
        echo "Using Python: $(which python)"
        echo "Python version: $(python --version)"
        echo "Upgrading pip, setuptools, and wheel..."
        python -m pip install --upgrade pip setuptools wheel
        
        echo "Installing dependencies with retry..."
        MAX_RETRIES=2
        RETRY_COUNT=0
        SUCCESS=false
        
        while [ $RETRY_COUNT -lt $MAX_RETRIES ] && [ "$SUCCESS" != "true" ]; do
          RETRY_COUNT=$((RETRY_COUNT + 1))
          echo "Attempt $RETRY_COUNT/$MAX_RETRIES: Installing dependencies..."
          
          if python -m pip install --no-cache-dir -r ./tests/requirements.txt; then
            SUCCESS=true
          else
            echo "Installation failed, trying with binary-only approach..."
            if python -m pip install --only-binary=:all: -r ./tests/requirements.txt; then
              SUCCESS=true
            else
              echo "Binary-only installation failed too."
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Will retry in 5 seconds..."
                sleep 5
              fi
            fi
          fi
        done
        
        if [ "$SUCCESS" != "true" ]; then
          echo "❌ Failed to install dependencies after $MAX_RETRIES attempts"
          exit 1
        fi
        
        echo "Running MLflow application tests"
        python ./tests/mlflow_test.py localhost:5000 --protocol http --connection-timeout 180 --debug
        EOF
        
        # Make the script executable
        chmod +x "$TMP_SCRIPT"
        
        # Run the script within the activated environment
        echo "Activating virtual environment and running tests..."
        source "$ACTIVATE_SCRIPT" && bash "$TMP_SCRIPT"
        
        # Clean up
        rm -f "$TMP_SCRIPT"
      
  # Task to clean the virtual environment
  venv:clean:
    desc: Clean up the Python virtual environment used for testing
    silent: true
    cmds:
      - echo "Cleaning up Python virtual environment..."
      - rm -rf "{{.TESTS_DIR}}/.venv"
      - echo "✅ Python virtual environment cleaned"

  # Documentation generation tasks
  helm:docs:generate:
    desc: Generate Helm chart documentation from templates
    silent: true
    cmds:
      - echo "Generating Helm chart documentation..."
      - |
        # Make sure helm-docs is installed
        if ! command -v helm-docs &> /dev/null; then
          echo "❌ helm-docs is not installed. Please install it from https://github.com/norwoodj/helm-docs"
          exit 1
        fi
        
        # Run helm-docs for each chart
        for chart in {{.CHARTS}}; do
          echo "Generating documentation for $chart chart..."
          cd {{.CHART_DIR}}/$chart && helm-docs -t README.md.gotmpl -t README_CHANGELOG.md.gotmpl -t README_CONFIG.md.gotmpl
        done
        
        echo "✅ Helm chart documentation generated successfully."

  helm:docs:verify:
    desc: Check if Helm chart documentation is up to date
    silent: true
    cmds:
      - echo "Checking if Helm chart documentation is up to date..."
      - |
        # Make sure helm-docs is installed
        if ! command -v helm-docs &> /dev/null; then
          echo "❌ helm-docs is not installed. Please install it from https://github.com/norwoodj/helm-docs"
          exit 1
        fi
        
        docs_outdated=false
        
        # For each chart, generate docs to a temp dir and compare with current docs
        for chart in {{.CHARTS}}; do
          echo "Checking documentation for $chart chart..."
          
          # Create temp directory
          tmp_dir=$(mktemp -d)
          trap 'rm -rf "$tmp_dir"' EXIT
          
          # Copy current README.md to temp dir
          readme_path="{{.CHART_DIR}}/$chart/README.md"
          tmp_readme="$tmp_dir/README.md"
          
          if [ -f "$readme_path" ]; then
            cp "$readme_path" "$tmp_readme"
          else
            echo "⚠️ README.md not found for $chart chart. This check will only be useful after docs are generated."
            touch "$tmp_readme"  # Create empty file for comparison
          fi
          
          # Generate fresh docs
          cd {{.CHART_DIR}}/$chart && helm-docs -t README.md.gotmpl -t README_CHANGELOG.md.gotmpl -t README_CONFIG.md.gotmpl -o "$tmp_dir"
          
          # Compare with current docs
          if [ -f "$readme_path" ] && ! diff -q "$readme_path" "$tmp_readme" > /dev/null; then
            echo "❌ Documentation for $chart chart is outdated. Run 'task helm:docs:generate' to update."
            docs_outdated=true
          else
            echo "✅ Documentation for $chart chart is up to date."
          fi
        done
        
        # Exit with error if any docs are outdated
        if [ "$docs_outdated" = true ]; then
          echo "❌ Some chart documentation files are outdated. Run 'task helm:docs:generate' to update them."
          exit 1
        else
          echo "✅ All chart documentation is up to date."
        fi

  # Version extraction
  extract:version:chart:
    desc: Extract and print the MLflow chart version
    silent: true
    cmds:
      - |
        echo "{{.MLFLOW_VERSION}}"

  # Namespace setup
  namespaces:create:
    desc: Create and setup required namespaces
    silent: true
    cmds:
      - echo "Setting up required namespaces..."
      - kubectl create namespace {{.NAMESPACE}} --dry-run=client -o yaml | kubectl apply -f -
      - echo "Namespace setup complete"
    status:
      - kubectl get namespace {{.NAMESPACE}} --no-headers --output=name 2>/dev/null

  # Registry authentication/setup
  registry:login:
    desc: Login to Replicated registry (requires REPLICATED_LICENSE_ID env var)
    silent: true
    cmds:
      - echo "Authenticating with Replicated registry..."
      - |
        if [ -z "$REPLICATED_LICENSE_ID" ]; then
          echo "ERROR: REPLICATED_LICENSE_ID environment variable must be set"
          exit 1
        fi
        helm registry login registry.replicated.com \
          --username="$REPLICATED_LICENSE_ID" \
          --password="$REPLICATED_LICENSE_ID"
      - echo "Registry login successful."

  # License retrieval and management
  license:get-id:
    desc: Extract license ID from a customer (requires REPLICATED_API_TOKEN and customer name)
    silent: true
    cmds:
      - echo "Extracting license ID for customer {{.CUSTOMER_NAME}}..."
      - |
        # Validate required environment variables
        if [ -z "$REPLICATED_API_TOKEN" ]; then
          echo "ERROR: REPLICATED_API_TOKEN environment variable must be set"
          exit 1
        fi
        
        if [ -z "{{.CUSTOMER_NAME}}" ]; then
          echo "ERROR: CUSTOMER_NAME parameter is required"
          exit 1
        fi
        
        # Run vendor-cli to inspect the customer and get the installation ID as JSON
        echo "Running vendor-cli to inspect customer..."
        CUSTOMER_JSON=$(docker run --rm \
          -e REPLICATED_API_TOKEN=$REPLICATED_API_TOKEN \
          -e REPLICATED_APP={{.APP_NAME}} \
          replicated/vendor-cli:latest \
          customer inspect --customer "{{.CUSTOMER_NAME}}" --output json)
        
        # Use jq to properly extract the installationId
        INSTALLATION_ID=$(echo "$CUSTOMER_JSON" | jq -r '.installationId')
        
        # Check if we got a valid ID
        if [ -z "$INSTALLATION_ID" ] || [ "$INSTALLATION_ID" = "null" ]; then
          echo "Failed to extract installationId from customer JSON"
          echo "JSON structure:" 
          echo "$CUSTOMER_JSON" | jq 'del(.installationId)' # Print JSON without the license ID
          exit 1
        fi
        
        # Print the license ID so it can be captured
        echo "$INSTALLATION_ID"

  # Download customer license
  license:download:
    desc: Download license for a customer (requires REPLICATED_API_TOKEN and customer name)
    silent: true
    cmds:
      - echo "Downloading license for customer {{.CUSTOMER_NAME}}..."
      - |
        # Validate required environment variables
        if [ -z "$REPLICATED_API_TOKEN" ]; then
          echo "ERROR: REPLICATED_API_TOKEN environment variable must be set"
          exit 1
        fi
        
        if [ -z "{{.CUSTOMER_NAME}}" ]; then
          echo "ERROR: CUSTOMER_NAME parameter is required"
          exit 1
        fi
        
        # Create a temporary directory for the license if it doesn't exist
        mkdir -p /tmp/replicated
        OUTPUT_FILE="/tmp/replicated/license-download-output.txt"
        LICENSE_FILE="/tmp/replicated/license.yaml"
        
        # Run vendor-cli to download the customer license to a temporary file first
        echo "Running vendor-cli to download license..."
        TMP_LICENSE_FILE=$(mktemp)
        set +e
        docker run --rm \
          -e REPLICATED_API_TOKEN=$REPLICATED_API_TOKEN \
          -e REPLICATED_APP={{.APP_NAME}} \
          replicated/vendor-cli:latest \
          customer download-license --customer "{{.CUSTOMER_NAME}}" > "$TMP_LICENSE_FILE" 2>$OUTPUT_FILE
        DOWNLOAD_EXIT_CODE=$?
        set -e
        
        if [ $DOWNLOAD_EXIT_CODE -ne 0 ]; then
          echo "ERROR: Failed to download license for customer {{.CUSTOMER_NAME}}"
          echo "Error output:"
          cat $OUTPUT_FILE
          rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"
          exit 1
        fi
        
        # Check if the file is empty
        if [ ! -s "$TMP_LICENSE_FILE" ]; then
          echo "ERROR: Downloaded license file is empty"
          cat $OUTPUT_FILE
          rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"
          exit 1
        fi
        
        # Verify the license file is valid YAML
        if command -v yq >/dev/null 2>&1; then
          echo "Validating license file is proper YAML..."
          if ! yq eval . "$TMP_LICENSE_FILE" > /dev/null 2>&1; then
            echo "ERROR: Downloaded license file is not valid YAML"
            echo "License file content:"
            cat "$TMP_LICENSE_FILE"
            rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"
            exit 1
          fi
        else
          echo "WARNING: yq not found, skipping YAML validation"
        fi
        
        # Remove any extra output or text before the YAML content
        # This extracts content between first '---' and the end of file
        if grep -q "^---" "$TMP_LICENSE_FILE"; then
          echo "License appears to be in YAML format with document marker, extracting YAML content..."
          sed -n '/^---/,$p' "$TMP_LICENSE_FILE" > "$LICENSE_FILE"
        else
          # If no '---' marker is found, check for '{' to identify JSON
          if grep -q "{" "$TMP_LICENSE_FILE"; then
            echo "License appears to be in JSON format, converting to YAML..."
            if command -v yq >/dev/null 2>&1; then
              cat "$TMP_LICENSE_FILE" | yq eval -P > "$LICENSE_FILE"
            else
              echo "ERROR: Cannot convert JSON to YAML without yq"
              cat "$TMP_LICENSE_FILE"
              rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"
              exit 1
            fi
          else
            # If neither YAML nor JSON markers are found, just copy the file
            echo "No YAML document marker or JSON found. Copying file as-is..."
            cat "$TMP_LICENSE_FILE" > "$LICENSE_FILE"
          fi
        fi
        
        # Log some debug information
        echo "License file content (first 5 lines):"
        head -n 5 "$LICENSE_FILE"
        
        # Verify file exists and has content
        if [ ! -s "$LICENSE_FILE" ]; then
          echo "ERROR: Final license file is empty after processing"
          rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"
          exit 1
        fi
        
        echo "License successfully downloaded to $LICENSE_FILE"
        rm -f $OUTPUT_FILE "$TMP_LICENSE_FILE"

  # Add the renamed MLflow service tasks
  helm:service:find:
    desc: Find the MLflow service in the namespace
    internal: true
    silent: true
    cmds:
      - echo "Looking for MLflow service in namespace {{.NAMESPACE}}..."
      - |
        # Wait for the MLflow service to be created
        echo "Waiting for MLflow service to be created..."
        MAX_RETRIES={{.WAIT_RETRIES}}
        RETRY_INTERVAL={{.RETRY_INTERVAL}}
        RETRY_COUNT=0
        SERVICE_FOUND=false
        
        while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
          echo "Check $((RETRY_COUNT+1))/$MAX_RETRIES: Looking for MLflow service..."
          if kubectl get svc mlflow -n {{.NAMESPACE}} --no-headers 2>/dev/null; then
            echo "✅ MLflow service found!"
            SERVICE_FOUND=true
            break
          else
            echo "MLflow service not found yet. Waiting $RETRY_INTERVAL seconds..."
            RETRY_COUNT=$((RETRY_COUNT+1))
            sleep $RETRY_INTERVAL
          fi
        done
        
        if [ "$SERVICE_FOUND" != "true" ]; then
          echo "❌ ERROR: MLflow service not found after $((MAX_RETRIES * RETRY_INTERVAL)) seconds."
          echo "Showing all available services in the namespace:"
          kubectl get svc -n {{.NAMESPACE}}
          echo "Showing pod status in the namespace:"
          kubectl get pods -n {{.NAMESPACE}}
          echo "Showing pod details:"
          kubectl describe pods -n {{.NAMESPACE}} -l app.kubernetes.io/name=mlflow
          exit 1
        fi
        
        # Verify the services are present
        echo "Verifying MLflow service exists..."
        kubectl get svc -n {{.NAMESPACE}}
        echo "✅ MLflow service verification completed."

  helm:pods:wait:
    desc: Wait for MLflow pods to be ready
    internal: true
    silent: true
    cmds:
      - echo "Checking MLflow pod status..."
      - |
        # Check pod status and wait for them to be running
        echo "Checking pod status..."
        kubectl get pods -n {{.NAMESPACE}}
        
        echo "Waiting for MLflow pods to be running..."
        kubectl wait --for=condition=Ready pods --selector=app.kubernetes.io/name=mlflow -n {{.NAMESPACE}} --timeout={{.TIMEOUT}} || {
          echo "WARNING: Timed out waiting for pods to be ready, will try port-forwarding anyway"
          kubectl describe pods -n {{.NAMESPACE}}
        }
        echo "✅ Pod readiness check completed."
