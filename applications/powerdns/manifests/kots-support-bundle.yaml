apiVersion: troubleshoot.sh/v1beta2
kind: SupportBundle
metadata:
  name: default
spec:
  collectors:
    - clusterInfo: {}
    - clusterResources: {}
    - logs:
        - name: powerdns-authoritative
          selector:
            - app.kubernetes.io/instance=powerdns-authoritative
    - logs:
        - name: powerdns-recursor
          selector:
            - app.kubernetes.io/instance=powerdns-recursor
    - logs:
        - name: powerdns-admin
          selector:
            - app.kubernetes.io/instance=powerdns-admin
    - ceph:
    - longhorn:
  analyzers:
    - deploymentStatus:
        namespace: default
        name: powerdns-authoritative
    - containerRuntime:
        outcomes:
          - fail:
              when: '== gvisor'
              message: The Admin Console does not support using the gvisor runtime
          - pass:
              message: A supported container runtime is present on all nodes
    - cephStatus: {}
    - longhorn: {}
    - clusterPodStatuses:
        outcomes:
          - fail:
              when: '!= Healthy'
              message: 'Status: {{ .Status.Reason }}'
    - statefulsetStatus: {}
    - deploymentStatus: {}
    - jobStatus: {}
    - replicasetStatus: {}
    - weaveReport:
    - textAnalyze:
        checkName: Inter-pod Networking
        exclude: ''
        ignoreIfNoFiles: true
        fileName: kots/goldpinger/*/kotsadm-*/goldpinger-statistics-stdout.txt
        outcomes:
          - fail:
              when: 'OK = false'
              message: Some nodes have pod communication issues
          - pass:
              message: Goldpinger can communicate properly
        regexGroups: '"OK": ?(?P<OK>\w+)'
    - textAnalyze:
        checkName: Etcdserver Database Size Exceeded
        exclude: ''
        ignoreIfNoFiles: true
        fileName: '*'
        regex: '(etcdserver)?.*mvcc.*database space exceeded'
        outcomes:
          - fail:
              when: 'true'
              message: 'etcdserver database has grown too large.  See: https://community.replicated.com/t/kubernetes-cluster-is-down-and-reporting-etcdserver-mvcc-database-size-exceeded/1428'
          - pass:
              when: 'false'
              message: etcdserver database is not too large
    - nodeResources:
        checkName: Node status check
        outcomes:
          - fail:
              when: 'nodeCondition(Ready) == False'
              message: 'Not all nodes are online.'
          - fail:
              when: 'nodeCondition(Ready) == Unknown'
              message: 'Not all nodes are online.'
          - pass:
              message: 'All nodes are online.'
    - clusterPodStatuses:
        checkName: contour pods unhealthy
        namespaces:
          - projectcontour
        outcomes:
          - fail:
              when: '!= Healthy' # Catch all unhealthy pods. A pod is considered healthy if it has a status of Completed, or Running and all of its containers are ready.
              message: A Contour pod, {{ .Name }}, is unhealthy with a status of {{ .Status.Reason }}. Restarting the pod may fix the issue.
    - textAnalyze:
        checkName: longhorn multipath conflict
        ignoreIfNoFiles: true
        fileName: longhorn/longhorn-system/logs/longhorn-csi-plugin-*/longhorn-csi-plugin.log
        outcomes:
          - fail:
              when: 'true'
              uri: 'https://longhorn.io/kb/troubleshooting-volume-with-multipath/'
              message: 'Longhorn volumes may be in use by system multipath.'
          - pass:
              when: 'false'
              message: 'No block-device conflicts detected'
        regex: '.*is apparently in use by the system;.*'
    - textAnalyze:
        checkName: Minio disk full
        fileName: cluster-resources/pods/logs/kurl/registry-*/registry.log
        ignoreIfNoFiles: true
        regex: '.*XMinioStorageFull: Storage backend has reached its minimum free disk threshold.*'
        outcomes:
          - fail:
              when: 'true'
              message: 'Minio Disk Full'
          - pass:
              when: 'false'
              message: 'Minio Disk Ok'
    - textAnalyze:
        checkName: Known issue with Rook < 1.4
        exclude: ''
        ignoreIfNoFiles: true
        fileName: /ceph/status.json
        regex: '\"ceph_release\": \"nautilus\"|\"status\": \"HEALTH_WARN\"'
        outcomes:
          - fail:
              when: 'true'
              message: 'If you have been removing and adding nodes then, you might want ensure that you are not facing the scenario described in the community topic: https://community.replicated.com/t/1099'
          - pass:
              when: 'false'
              message: 'You are not using a Rook versions < 1.4 and/or your Ceph status is OK'
    - textAnalyze:
        checkName: Rook rbd filesystem consistency
        fileName: /kots/rook/rook-ceph-agent-*.log
        ignoreIfNoFiles: true
        regex: 'UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY.'
        outcomes:
          - fail:
              when: 'true'
              message: 'One or more rook rbd(s) were detected to have filesystem inconsistencies and require manual intervention'
          - pass:
              when: 'false'
              message: 'Rook filesystem consistency ok'
    - jsonCompare:
        checkName: https://replicated.app host health check
        fileName: replicated.app-health-check.json
        path: 'response.status'
        value: '200'
        outcomes:
          - fail:
              when: 'false'
              message: https://replicated.app is unhealthy. License and software update checks from replicated will fail. If this is locked down environment, please check your proxy settings.
              uri: https://kurl.sh/docs/install-with-kurl/proxy-installs
          - pass:
              when: 'true'
              message: https://replicated.app host is healthy
    - storageClass:
        checkName: Check for default storage class
        outcomes:
          - fail:
              message: No default storage class found
          - pass:
              message: Default storage class found
    - nodeMetrics:
        checkName: Check for PVCs using more than 80% storage in the entire cluster
        outcomes:
          - fail:
              when: 'pvcUsedPercentage >= 80'
              message: 'There are PVCs using more than 80% of storage: {{ .PVC.ConcatenatedNames }}'
          - pass:
              message: 'No PVCs are using more than 80% of storage'
